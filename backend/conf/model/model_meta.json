{
    "provider2models": {
        "Claude": {
            "default": {
                "display_info": {
                    "output_tokens": 4096,
                    "max_tokens": 131072
                },
                "capability": {
                    "function_call": true,
                    "image_understanding": true,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": true
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "Temperature",
                        "desc": "**Temperature**:\n\n- When you increase this value, the model outputs more diverse and innovative content; when you decrease it, the model outputs less diverse content that strictly follows the given instructions.\n- It is recommended not to adjust this value with \"Top p\" at the same time.",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 1,
                        "default_val": {
                            "default_val": "1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "Response max length",
                        "desc": "You can specify the maximum length of the tokens output through this value. Typically, 100 tokens are approximately equal to 150 Chinese characters.",
                        "type": 2,
                        "min": "5",
                        "max": "4096",
                        "precision": 0,
                        "default_val": {
                            "default_val": "1024"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "Input and output settings"
                        }
                    }
                ]
            },
            "claude-sonnet-4-5": {
                "display_info": {
                    "name": "Claude 4.5",
                    "description": {},
                    "output_tokens": 64000,
                    "max_tokens": 1000000
                },
                "capability": {
                    "function_call": true,
                    "image_understanding": true,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": true
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "Temperature",
                        "desc": "**Temperature**:\n\n- When you increase this value, the model outputs more diverse and innovative content; when you decrease it, the model outputs less diverse content that strictly follows the given instructions.\n- It is recommended not to adjust this value with \"Top p\" at the same time.",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 1,
                        "default_val": {
                            "default_val": "1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "Response max length",
                        "desc": "You can specify the maximum length of the tokens output through this value. Typically, 100 tokens are approximately equal to 150 Chinese characters.",
                        "type": 2,
                        "min": "5",
                        "max": "4096",
                        "precision": 0,
                        "default_val": {
                            "default_val": "1024"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "Input and output settings"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "Current time",
                        "desc": "The current accurate time will be appended to each user query after enabled. [GuideDoc](https://www.coze.com/open/docs/guides/llm#3a97d6f3)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "Default instruction"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "Prompt leakage prevention",
                        "desc": "The system prompt will be reinforced after enabled, which can significantly reduce the probability of system prompt leakage. [GuideDoc](https://www.coze.com/open/docs/guides/llm#3a97d6f3)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "Default instruction"
                        }
                    }
                ]
            },
            "claude-sonnet-4-5-20250929": {
                "display_info": {
                    "name": "Claude 4.5",
                    "description": {},
                    "output_tokens": 64000,
                    "max_tokens": 1000000
                },
                "capability": {
                    "function_call": true,
                    "image_understanding": true,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": true
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "Temperature",
                        "desc": "**Temperature**:\n\n- When you increase this value, the model outputs more diverse and innovative content; when you decrease it, the model outputs less diverse content that strictly follows the given instructions.\n- It is recommended not to adjust this value with \"Top p\" at the same time.",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 1,
                        "default_val": {
                            "default_val": "1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "Response max length",
                        "desc": "You can specify the maximum length of the tokens output through this value. Typically, 100 tokens are approximately equal to 150 Chinese characters.",
                        "type": 2,
                        "min": "5",
                        "max": "4096",
                        "precision": 0,
                        "default_val": {
                            "default_val": "1024"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "Input and output settings"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "Current time",
                        "desc": "The current accurate time will be appended to each user query after enabled. [GuideDoc](https://www.coze.com/open/docs/guides/llm#3a97d6f3)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "Default instruction"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "Prompt leakage prevention",
                        "desc": "The system prompt will be reinforced after enabled, which can significantly reduce the probability of system prompt leakage. [GuideDoc](https://www.coze.com/open/docs/guides/llm#3a97d6f3)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "Default instruction"
                        }
                    }
                ]
            },
            "claude-3-5-sonnet-20240620": {
                "display_info": {
                    "name": "Claude 3.5 Sonnet",
                    "description": {
                        "zh_cn": "excels in text and code generation",
                        "en_us": "excels in text and code generation"
                    },
                    "output_tokens": 4096,
                    "max_tokens": 200000
                },
                "capability": {
                    "function_call": true,
                    "image_understanding": true,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": true
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "Temperature",
                        "desc": "**Temperature**:\n\n- When you increase this value, the model outputs more diverse and innovative content; when you decrease it, the model outputs less diverse content that strictly follows the given instructions.\n- It is recommended not to adjust this value with \"Top p\" at the same time.",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 1,
                        "default_val": {
                            "default_val": "1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "Response max length",
                        "desc": "You can specify the maximum length of the tokens output through this value. Typically, 100 tokens are approximately equal to 150 Chinese characters.",
                        "type": 2,
                        "min": "5",
                        "max": "4096",
                        "precision": 0,
                        "default_val": {
                            "default_val": "4096"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "Input and output settings"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "Current time",
                        "desc": "The current accurate time will be appended to each user query after enabled. [GuideDoc](https://www.coze.com/open/docs/guides/llm#3a97d6f3)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "Default instruction"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "Prompt leakage prevention",
                        "desc": "The system prompt will be reinforced after enabled, which can significantly reduce the probability of system prompt leakage. [GuideDoc](https://www.coze.com/open/docs/guides/llm#3a97d6f3)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "Default instruction"
                        }
                    }
                ]
            },
            "claude-3-5-sonnet-20241022": {
                "display_info": {
                    "name": "Claude 3.5 Sonnet v2",
                    "description": {},
                    "output_tokens": 4096,
                    "max_tokens": 131072
                },
                "capability": {
                    "function_call": true,
                    "image_understanding": true,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": true
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "Temperature",
                        "desc": "**Temperature**:\n\n- When you increase this value, the model outputs more diverse and innovative content; when you decrease it, the model outputs less diverse content that strictly follows the given instructions.\n- It is recommended not to adjust this value with \"Top p\" at the same time.",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 1,
                        "default_val": {
                            "default_val": "1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "Response max length",
                        "desc": "You can specify the maximum length of the tokens output through this value. Typically, 100 tokens are approximately equal to 150 Chinese characters.",
                        "type": 2,
                        "min": "5",
                        "max": "4096",
                        "precision": 0,
                        "default_val": {
                            "default_val": "1024"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "Input and output settings"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "Current time",
                        "desc": "The current accurate time will be appended to each user query after enabled. [GuideDoc](https://www.coze.com/open/docs/guides/llm#3a97d6f3)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "Default instruction"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "Prompt leakage prevention",
                        "desc": "The system prompt will be reinforced after enabled, which can significantly reduce the probability of system prompt leakage. [GuideDoc](https://www.coze.com/open/docs/guides/llm#3a97d6f3)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "Default instruction"
                        }
                    }
                ]
            },
            "claude-3-haiku-20240307": {
                "display_info": {
                    "name": "Claude 3 Haiku",
                    "description": {},
                    "output_tokens": 4096,
                    "max_tokens": 48000
                },
                "capability": {
                    "function_call": true,
                    "image_understanding": false,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": false
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "Temperature",
                        "desc": "**Temperature**:\n\n- When you increase this value, the model outputs more diverse and innovative content; when you decrease it, the model outputs less diverse content that strictly follows the given instructions.\n- It is recommended not to adjust this value with \"Top p\" at the same time.",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 1,
                        "default_val": {
                            "default_val": "1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "Response max length",
                        "desc": "You can specify the maximum length of the tokens output through this value. Typically, 100 tokens are approximately equal to 150 Chinese characters.",
                        "type": 2,
                        "min": "1",
                        "max": "4096",
                        "precision": 0,
                        "default_val": {
                            "default_val": "4096"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "Input and output settings"
                        }
                    },
                    {
                        "name": "response_format",
                        "label": "Output format",
                        "desc": "**Output Format**:\n\n- **Text**: Replies in plain text format\n- **Markdown**: Uses Markdown format for replies\n- **JSON**: Uses JSON format for replies",
                        "type": 2,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "0"
                        },
                        "options": [
                            {
                                "label": "Text",
                                "value": "0"
                            },
                            {
                                "label": "Markdown",
                                "value": "1"
                            },
                            {
                                "label": "JSON",
                                "value": "2"
                            }
                        ],
                        "param_class": {
                            "class_id": 2,
                            "label": "Input and output settings"
                        }
                    }
                ]
            },
            "claude-3-5-haiku-20241022": {
                "display_info": {
                    "name": "Claude 3 Haiku",
                    "description": {
                        "zh_cn": "delivering near-instant responses",
                        "en_us": "delivering near-instant responses"
                    },
                    "output_tokens": 4096,
                    "max_tokens": 200000
                },
                "capability": {
                    "function_call": true,
                    "image_understanding": true,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": true
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "Temperature",
                        "desc": "**Temperature**:\n\n- When you increase this value, the model outputs more diverse and innovative content; when you decrease it, the model outputs less diverse content that strictly follows the given instructions.\n- It is recommended not to adjust this value with \"Top p\" at the same time.",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 1,
                        "default_val": {
                            "default_val": "1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "Response max length",
                        "desc": "You can specify the maximum length of the tokens output through this value. Typically, 100 tokens are approximately equal to 150 Chinese characters.",
                        "type": 2,
                        "min": "1",
                        "max": "4096",
                        "precision": 0,
                        "default_val": {
                            "default_val": "4096"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "Input and output settings"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "Current time",
                        "desc": "The current accurate time will be appended to each user query after enabled. [GuideDoc](https://www.coze.com/open/docs/guides/llm#3a97d6f3)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "Default instruction"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "Prompt leakage prevention",
                        "desc": "The system prompt will be reinforced after enabled, which can significantly reduce the probability of system prompt leakage. [GuideDoc](https://www.coze.com/open/docs/guides/llm#3a97d6f3)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "Default instruction"
                        }
                    }
                ]
            },
            "claude-3-opus-20240229": {
                "display_info": {
                    "name": "Claude 3 Opus",
                    "description": {},
                    "output_tokens": 4096,
                    "max_tokens": 200000
                },
                "capability": {
                    "function_call": true,
                    "image_understanding": true,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": true
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "Temperature",
                        "desc": "**Temperature**:\n\n- When you increase this value, the model outputs more diverse and innovative content; when you decrease it, the model outputs less diverse content that strictly follows the given instructions.\n- It is recommended not to adjust this value with \"Top p\" at the same time.",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 1,
                        "default_val": {
                            "default_val": "1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "Response max length",
                        "desc": "You can specify the maximum length of the tokens output through this value. Typically, 100 tokens are approximately equal to 150 Chinese characters.",
                        "type": 2,
                        "min": "1",
                        "max": "4096",
                        "precision": 0,
                        "default_val": {
                            "default_val": "4096"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "Input and output settings"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "Current time",
                        "desc": "The current accurate time will be appended to each user query after enabled. [GuideDoc](https://www.coze.com/open/docs/guides/llm#3a97d6f3)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "Default instruction"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "Prompt leakage prevention",
                        "desc": "The system prompt will be reinforced after enabled, which can significantly reduce the probability of system prompt leakage. [GuideDoc](https://www.coze.com/open/docs/guides/llm#3a97d6f3)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "Default instruction"
                        }
                    }
                ]
            },
            "claude-3-sonnet-20240229": {
                "display_info": {
                    "name": "Claude 3 Sonnet",
                    "description": {},
                    "output_tokens": 4096,
                    "max_tokens": 200000
                },
                "capability": {
                    "function_call": true,
                    "image_understanding": true,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": true
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "Temperature",
                        "desc": "**Temperature**:\n\n- When you increase this value, the model outputs more diverse and innovative content; when you decrease it, the model outputs less diverse content that strictly follows the given instructions.\n- It is recommended not to adjust this value with \"Top p\" at the same time.",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 1,
                        "default_val": {
                            "default_val": "1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "Response max length",
                        "desc": "You can specify the maximum length of the tokens output through this value. Typically, 100 tokens are approximately equal to 150 Chinese characters.",
                        "type": 2,
                        "min": "1",
                        "max": "4096",
                        "precision": 0,
                        "default_val": {
                            "default_val": "4096"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "Input and output settings"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "Current time",
                        "desc": "The current accurate time will be appended to each user query after enabled. [GuideDoc](https://www.coze.com/open/docs/guides/llm#3a97d6f3)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "Default instruction"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "Prompt leakage prevention",
                        "desc": "The system prompt will be reinforced after enabled, which can significantly reduce the probability of system prompt leakage. [GuideDoc](https://www.coze.com/open/docs/guides/llm#3a97d6f3)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "Default instruction"
                        }
                    }
                ]
            },
            "claude-3-7-sonnet-20250219": {
                "display_info": {
                    "name": "Claude 3.7 Sonnet",
                    "description": {
                        "zh_cn": "excels in text and code generation",
                        "en_us": "excels in text and code generation"
                    },
                    "output_tokens": 4096,
                    "max_tokens": 204800
                },
                "capability": {
                    "cot_display": true,
                    "function_call": true,
                    "image_understanding": true,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": true
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "Temperature",
                        "desc": "**Temperature**:\n\n- When you increase this value, the model outputs more diverse and innovative content; when you decrease it, the model outputs less diverse content that strictly follows the given instructions.\n- It is recommended not to adjust this value with \"Top p\" at the same time.",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 1,
                        "default_val": {
                            "default_val": "1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "Response max length",
                        "desc": "You can specify the maximum length of the tokens output through this value. Typically, 100 tokens are approximately equal to 150 Chinese characters.",
                        "type": 2,
                        "min": "5",
                        "max": "8192",
                        "precision": 0,
                        "default_val": {
                            "default_val": "1025"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "Input and output settings"
                        }
                    },
                    {
                        "name": "thinking_type",
                        "label": "Thinking",
                        "desc": "After enabling deep thinking, before outputting the final answer, the model will first generate a segment of thought chain content to enhance the accuracy of the final response.",
                        "type": 4,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "disabled"
                        },
                        "options": [
                            {
                                "label": "enabled",
                                "value": "enabled"
                            },
                            {
                                "label": "disabled",
                                "value": "disabled"
                            }
                        ],
                        "param_class": {
                            "class_id": 6,
                            "label": "Deep Thinking"
                        }
                    },
                    {
                        "name": "thinking_budget_tokens",
                        "label": "Thinking Budget Tokens",
                        "desc": "Adjusting the output length of model's thinking result.",
                        "type": 2,
                        "min": "1024",
                        "max": "4096",
                        "precision": 0,
                        "default_val": {
                            "default_val": "1024"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 6,
                            "label": "Deep Thinking"
                        }
                    }
                ]
            }
        },
        "GPT": {
            "default": {
                "display_info": {
                    "output_tokens": 4096,
                    "max_tokens": 400000
                },
                "capability": {
                    "cot_display": false,
                    "function_call": true,
                    "image_understanding": false,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": false,
                    "prefill_resp": false
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "Temperature",
                        "desc": "**Temperature**:\n\n- When you increase this value, the model outputs more diverse and innovative content; when you decrease it, the model outputs less diverse content that strictly follows the given instructions.\n- It is recommended not to adjust this value with \"Top p\" at the same time.",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 1,
                        "default_val": {
                            "default_val": "1",
                            "creative": "1",
                            "balance": "0.8",
                            "precise": "0.3"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "Response max length",
                        "desc": "You can specify the maximum length of the tokens output through this value. Typically, 100 tokens are approximately equal to 150 Chinese characters.",
                        "type": 2,
                        "min": "1",
                        "max": "4096",
                        "precision": 0,
                        "default_val": {
                            "default_val": "4096"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "Input and output settings"
                        }
                    }
                ]
            },
            "gpt-3.5-turbo-0125": {
                "display_info": {
                    "name": "GPT-3.5 Turbo 0125",
                    "description": {},
                    "output_tokens": 4096,
                    "max_tokens": 16385
                },
                "capability": {
                    "function_call": true,
                    "image_understanding": false,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": false
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "Temperature",
                        "desc": "**Temperature**:\n\n- When you increase this value, the model outputs more diverse and innovative content; when you decrease it, the model outputs less diverse content that strictly follows the given instructions.\n- It is recommended not to adjust this value with \"Top p\" at the same time.",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 1,
                        "default_val": {
                            "default_val": "1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "Response max length",
                        "desc": "You can specify the maximum length of the tokens output through this value. Typically, 100 tokens are approximately equal to 150 Chinese characters.",
                        "type": 2,
                        "min": "1",
                        "max": "4096",
                        "precision": 0,
                        "default_val": {
                            "default_val": "4096"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "Input and output settings"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "Current time",
                        "desc": "The current accurate time will be appended to each user query after enabled. [GuideDoc](https://www.coze.com/open/docs/guides/llm#3a97d6f3)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "Default instruction"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "Prompt leakage prevention",
                        "desc": "The system prompt will be reinforced after enabled, which can significantly reduce the probability of system prompt leakage. [GuideDoc](https://www.coze.com/open/docs/guides/llm#3a97d6f3)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "Default instruction"
                        }
                    }
                ]
            },
            "gpt-4-turbo-2024-04-09": {
                "display_info": {
                    "name": "GPT-4 Turbo",
                    "description": {
                        "zh_cn": "Will be deprecated soon",
                        "en_us": "Will be deprecated soon"
                    },
                    "output_tokens": 4096,
                    "max_tokens": 128000
                },
                "capability": {
                    "function_call": true,
                    "image_understanding": false,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": false
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "Temperature",
                        "desc": "**Temperature**:\n\n- When you increase this value, the model outputs more diverse and innovative content; when you decrease it, the model outputs less diverse content that strictly follows the given instructions.\n- It is recommended not to adjust this value with \"Top p\" at the same time.",
                        "type": 1,
                        "min": "0",
                        "max": "2",
                        "precision": 2,
                        "default_val": {
                            "default_val": "1",
                            "creative": "0.8",
                            "balance": "0.5",
                            "precise": "0.1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "top_p",
                        "label": "Top p",
                        "desc": "**Top P**:\n\n- An alternative to sampling with temperature, where only tokens within the top p probability mass are considered. For example, 0.1 means only the top 10% probability mass tokens are considered.\n- We recommend altering this or temperature, but not both.",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 2,
                        "default_val": {
                            "default_val": "1",
                            "creative": "1",
                            "balance": "1",
                            "precise": "1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "frequency_penalty",
                        "label": "Frequency penalty",
                        "desc": "**Frequency Penalty**: When positive, it discourages the model from repeating the same words and phrases, thereby increasing the diversity of the output.",
                        "type": 1,
                        "min": "-2",
                        "max": "2",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0",
                            "creative": "0",
                            "balance": "0",
                            "precise": "0"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "presence_penalty",
                        "label": "Presence penalty",
                        "desc": "**Presence Penalty**: When positive, it prevents the model from discussing the same topics repeatedly, thereby increasing the diversity of the output.",
                        "type": 1,
                        "min": "-2",
                        "max": "2",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0",
                            "creative": "0",
                            "balance": "0",
                            "precise": "0"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "Response max length",
                        "desc": "You can specify the maximum length of the tokens output through this value. Typically, 100 tokens are approximately equal to 150 Chinese characters.",
                        "type": 2,
                        "min": "1",
                        "max": "4096",
                        "precision": 0,
                        "default_val": {
                            "default_val": "2048"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "Input and output settings"
                        }
                    },
                    {
                        "name": "response_format",
                        "label": "Output format",
                        "desc": "**Output Format**:\n\n- **Text**: Replies in plain text format\n- **Markdown**: Uses Markdown format for replies\n- **JSON**: Uses JSON format for replies",
                        "type": 2,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "0"
                        },
                        "options": [
                            {
                                "label": "Text",
                                "value": "0"
                            },
                            {
                                "label": "Markdown",
                                "value": "1"
                            },
                            {
                                "label": "JSON",
                                "value": "2"
                            }
                        ],
                        "param_class": {
                            "class_id": 2,
                            "label": "Input and output settings"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "Current time",
                        "desc": "The current accurate time will be appended to each user query after enabled. [GuideDoc](https://www.coze.com/open/docs/guides/llm#3a97d6f3)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "Default instruction"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "Prompt leakage prevention",
                        "desc": "The system prompt will be reinforced after enabled, which can significantly reduce the probability of system prompt leakage. [GuideDoc](https://www.coze.com/open/docs/guides/llm#3a97d6f3)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "Default instruction"
                        }
                    }
                ]
            },
            "gpt-4o-2024-05-13": {
                "display_info": {
                    "name": "GPT-4o",
                    "description": {
                        "zh_cn": "Multi-modal, 320ms, 88.7% MMLU, excels in education, customer support, health, and entertainment.",
                        "en_us": "Multi-modal, 320ms, 88.7% MMLU, excels in education, customer support, health, and entertainment."
                    },
                    "output_tokens": 4096,
                    "max_tokens": 8192
                },
                "capability": {
                    "function_call": true,
                    "image_understanding": true,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": true
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "Temperature",
                        "desc": "**Temperature**:\n\n- When you increase this value, the model outputs more diverse and innovative content; when you decrease it, the model outputs less diverse content that strictly follows the given instructions.\n- It is recommended not to adjust this value with \"Top p\" at the same time.",
                        "type": 1,
                        "min": "0",
                        "max": "2",
                        "precision": 2,
                        "default_val": {
                            "default_val": "1",
                            "creative": "0.8",
                            "balance": "0.5",
                            "precise": "0.1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "top_p",
                        "label": "Top p",
                        "desc": "**Top P**:\n\n- An alternative to sampling with temperature, where only tokens within the top p probability mass are considered. For example, 0.1 means only the top 10% probability mass tokens are considered.\n- We recommend altering this or temperature, but not both.",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 2,
                        "default_val": {
                            "default_val": "1",
                            "creative": "1",
                            "balance": "1",
                            "precise": "1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "frequency_penalty",
                        "label": "Frequency penalty",
                        "desc": "**Frequency Penalty**: When positive, it discourages the model from repeating the same words and phrases, thereby increasing the diversity of the output.",
                        "type": 1,
                        "min": "-2",
                        "max": "2",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0",
                            "creative": "0",
                            "balance": "0",
                            "precise": "0"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "presence_penalty",
                        "label": "Presence penalty",
                        "desc": "**Presence Penalty**: When positive, it prevents the model from discussing the same topics repeatedly, thereby increasing the diversity of the output.",
                        "type": 1,
                        "min": "-2",
                        "max": "2",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0",
                            "creative": "0",
                            "balance": "0",
                            "precise": "0"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "Response max length",
                        "desc": "You can specify the maximum length of the tokens output through this value. Typically, 100 tokens are approximately equal to 150 Chinese characters.",
                        "type": 2,
                        "min": "5",
                        "max": "4096",
                        "precision": 0,
                        "default_val": {
                            "default_val": "1024"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "Input and output settings"
                        }
                    },
                    {
                        "name": "response_format",
                        "label": "Output format",
                        "desc": "**Output Format**:\n\n- **Text**: Replies in plain text format\n- **Markdown**: Uses Markdown format for replies\n- **JSON**: Uses JSON format for replies",
                        "type": 2,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "0"
                        },
                        "options": [
                            {
                                "label": "Text",
                                "value": "0"
                            },
                            {
                                "label": "Markdown",
                                "value": "1"
                            },
                            {
                                "label": "JSON",
                                "value": "2"
                            }
                        ],
                        "param_class": {
                            "class_id": 2,
                            "label": "Input and output settings"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "Current time",
                        "desc": "The current accurate time will be appended to each user query after enabled. [GuideDoc](https://www.coze.com/open/docs/guides/llm#3a97d6f3)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "Default instruction"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "Prompt leakage prevention",
                        "desc": "The system prompt will be reinforced after enabled, which can significantly reduce the probability of system prompt leakage. [GuideDoc](https://www.coze.com/open/docs/guides/llm#3a97d6f3)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "Default instruction"
                        }
                    }
                ]
            },
            "gpt-4o-2024-08-06": {
                "display_info": {
                    "name": "GPT-4o",
                    "description": {
                        "zh_cn": "Multi-modal, 320ms, 88.7% MMLU, excels in education, customer support, health, and entertainment.",
                        "en_us": "Multi-modal, 320ms, 88.7% MMLU, excels in education, customer support, health, and entertainment."
                    },
                    "output_tokens": 8192,
                    "max_tokens": 131072
                },
                "capability": {
                    "function_call": true,
                    "image_understanding": true,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": true
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "Temperature",
                        "desc": "**Temperature**:\n\n- When you increase this value, the model outputs more diverse and innovative content; when you decrease it, the model outputs less diverse content that strictly follows the given instructions.\n- It is recommended not to adjust this value with \"Top p\" at the same time.",
                        "type": 1,
                        "min": "0",
                        "max": "2",
                        "precision": 2,
                        "default_val": {
                            "default_val": "1",
                            "creative": "0.8",
                            "balance": "0.5",
                            "precise": "0.1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "top_p",
                        "label": "Top p",
                        "desc": "**Top P**:\n\n- An alternative to sampling with temperature, where only tokens within the top p probability mass are considered. For example, 0.1 means only the top 10% probability mass tokens are considered.\n- We recommend altering this or temperature, but not both.",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 2,
                        "default_val": {
                            "default_val": "1",
                            "creative": "1",
                            "balance": "1",
                            "precise": "1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "frequency_penalty",
                        "label": "Frequency penalty",
                        "desc": "**Frequency Penalty**: When positive, it discourages the model from repeating the same words and phrases, thereby increasing the diversity of the output.",
                        "type": 1,
                        "min": "-2",
                        "max": "2",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0",
                            "creative": "0",
                            "balance": "0",
                            "precise": "0"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "presence_penalty",
                        "label": "Presence penalty",
                        "desc": "**Presence Penalty**: When positive, it prevents the model from discussing the same topics repeatedly, thereby increasing the diversity of the output.",
                        "type": 1,
                        "min": "-2",
                        "max": "2",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0",
                            "creative": "0",
                            "balance": "0",
                            "precise": "0"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "Response max length",
                        "desc": "You can specify the maximum length of the tokens output through this value. Typically, 100 tokens are approximately equal to 150 Chinese characters.",
                        "type": 2,
                        "min": "5",
                        "max": "8192",
                        "precision": 0,
                        "default_val": {
                            "default_val": "1024"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "Input and output settings"
                        }
                    },
                    {
                        "name": "response_format",
                        "label": "Output format",
                        "desc": "**Output Format**:\n\n- **Text**: Replies in plain text format\n- **Markdown**: Uses Markdown format for replies\n- **JSON**: Uses JSON format for replies",
                        "type": 2,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "0"
                        },
                        "options": [
                            {
                                "label": "Text",
                                "value": "0"
                            },
                            {
                                "label": "Markdown",
                                "value": "1"
                            },
                            {
                                "label": "JSON",
                                "value": "2"
                            }
                        ],
                        "param_class": {
                            "class_id": 2,
                            "label": "Input and output settings"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "Current time",
                        "desc": "The current accurate time will be appended to each user query after enabled. [GuideDoc](https://www.coze.com/open/docs/guides/llm#3a97d6f3)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "Default instruction"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "Prompt leakage prevention",
                        "desc": "The system prompt will be reinforced after enabled, which can significantly reduce the probability of system prompt leakage. [GuideDoc](https://www.coze.com/open/docs/guides/llm#3a97d6f3)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "Default instruction"
                        }
                    }
                ]
            },
            "gpt-4o-mini-2024-07-18": {
                "display_info": {
                    "name": "GPT-4o mini",
                    "description": {
                        "zh_cn": "Lightweight, multi-modal (82% MMLU), cost-effective.",
                        "en_us": "Lightweight, multi-modal (82% MMLU), cost-effective."
                    },
                    "output_tokens": 4096,
                    "max_tokens": 131072
                },
                "capability": {
                    "function_call": true,
                    "image_understanding": true,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": true
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "Temperature",
                        "desc": "**Temperature**:\n\n- When you increase this value, the model outputs more diverse and innovative content; when you decrease it, the model outputs less diverse content that strictly follows the given instructions.\n- It is recommended not to adjust this value with \"Top p\" at the same time.",
                        "type": 1,
                        "min": "0",
                        "max": "2",
                        "precision": 2,
                        "default_val": {
                            "default_val": "1",
                            "creative": "0.8",
                            "balance": "0.5",
                            "precise": "0.1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "top_p",
                        "label": "Top p",
                        "desc": "**Top P**:\n\n- An alternative to sampling with temperature, where only tokens within the top p probability mass are considered. For example, 0.1 means only the top 10% probability mass tokens are considered.\n- We recommend altering this or temperature, but not both.",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 2,
                        "default_val": {
                            "default_val": "1",
                            "creative": "1",
                            "balance": "1",
                            "precise": "1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "frequency_penalty",
                        "label": "Frequency penalty",
                        "desc": "**Frequency Penalty**: When positive, it discourages the model from repeating the same words and phrases, thereby increasing the diversity of the output.",
                        "type": 1,
                        "min": "-2",
                        "max": "2",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0",
                            "creative": "0",
                            "balance": "0",
                            "precise": "0"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "presence_penalty",
                        "label": "Presence penalty",
                        "desc": "**Presence Penalty**: When positive, it prevents the model from discussing the same topics repeatedly, thereby increasing the diversity of the output.",
                        "type": 1,
                        "min": "-2",
                        "max": "2",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0",
                            "creative": "0",
                            "balance": "0",
                            "precise": "0"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "Response max length",
                        "desc": "You can specify the maximum length of the tokens output through this value. Typically, 100 tokens are approximately equal to 150 Chinese characters.",
                        "type": 2,
                        "min": "5",
                        "max": "8192",
                        "precision": 0,
                        "default_val": {
                            "default_val": "1024"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "Input and output settings"
                        }
                    },
                    {
                        "name": "response_format",
                        "label": "Output format",
                        "desc": "**Output Format**:\n\n- **Text**: Replies in plain text format\n- **Markdown**: Uses Markdown format for replies\n- **JSON**: Uses JSON format for replies",
                        "type": 2,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "0"
                        },
                        "options": [
                            {
                                "label": "Text",
                                "value": "0"
                            },
                            {
                                "label": "Markdown",
                                "value": "1"
                            },
                            {
                                "label": "JSON",
                                "value": "2"
                            }
                        ],
                        "param_class": {
                            "class_id": 2,
                            "label": "Input and output settings"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "Current time",
                        "desc": "The current accurate time will be appended to each user query after enabled. [GuideDoc](https://www.coze.com/open/docs/guides/llm#3a97d6f3)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "Default instruction"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "Prompt leakage prevention",
                        "desc": "The system prompt will be reinforced after enabled, which can significantly reduce the probability of system prompt leakage. [GuideDoc](https://www.coze.com/open/docs/guides/llm#3a97d6f3)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "Default instruction"
                        }
                    }
                ]
            },
            "gpt-5-2025-08-07": {
                "display_info": {
                    "name": "gpt-5-2025-08-07",
                    "description": {
                        "zh_cn": "gpt-5-2025-08-07\t",
                        "en_us": "gpt-5-2025-08-07\t"
                    },
                    "output_tokens": 4096,
                    "max_tokens": 400000
                },
                "capability": {
                    "cot_display": false,
                    "function_call": true,
                    "image_understanding": false,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": false,
                    "prefill_resp": false
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "Temperature",
                        "desc": "**Temperature**:\n\n- When you increase this value, the model outputs more diverse and innovative content; when you decrease it, the model outputs less diverse content that strictly follows the given instructions.\n- It is recommended not to adjust this value with \"Top p\" at the same time.",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 1,
                        "default_val": {
                            "default_val": "1",
                            "creative": "1",
                            "balance": "0.8",
                            "precise": "0.3"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "Response max length",
                        "desc": "You can specify the maximum length of the tokens output through this value. Typically, 100 tokens are approximately equal to 150 Chinese characters.",
                        "type": 2,
                        "min": "1",
                        "max": "128000",
                        "precision": 0,
                        "default_val": {
                            "default_val": "4096"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "Input and output settings"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "Current time",
                        "desc": "The current accurate time will be appended to each user query after enabled. [GuideDoc](https://www.coze.com/open/docs/guides/llm#3a97d6f3)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "Default instruction"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "Prompt leakage prevention",
                        "desc": "The system prompt will be reinforced after enabled, which can significantly reduce the probability of system prompt leakage. [GuideDoc](https://www.coze.com/open/docs/guides/llm#3a97d6f3)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "Default instruction"
                        }
                    }
                ]
            }
        },
        "Gemini": {
            "default": {
                "display_info": {
                    "output_tokens": 4096,
                    "max_tokens": 204800
                },
                "capability": {
                    "function_call": true,
                    "image_understanding": true,
                    "video_understanding": true,
                    "audio_understanding": true,
                    "support_multi_modal": true
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "Temperature",
                        "desc": "**Temperature**:\n\n- When you increase this value, the model outputs more diverse and innovative content; when you decrease it, the model outputs less diverse content that strictly follows the given instructions.\n- It is recommended not to adjust this value with \"Top p\" at the same time.",
                        "type": 1,
                        "min": "0",
                        "max": "2",
                        "precision": 2,
                        "default_val": {
                            "default_val": "1",
                            "creative": "0.8",
                            "balance": "0.5",
                            "precise": "0.1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "top_p",
                        "label": "Top p",
                        "desc": "**Top P**:\n\n- An alternative to sampling with temperature, where only tokens within the top p probability mass are considered. For example, 0.1 means only the top 10% probability mass tokens are considered.\n- We recommend altering this or temperature, but not both.",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0.94",
                            "creative": "1",
                            "balance": "1",
                            "precise": "1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "Response max length",
                        "desc": "You can specify the maximum length of the tokens output through this value. Typically, 100 tokens are approximately equal to 150 Chinese characters.",
                        "type": 2,
                        "min": "1",
                        "max": "8192",
                        "precision": 0,
                        "default_val": {
                            "default_val": "1024"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "Input and output settings"
                        }
                    },
                    {
                        "name": "response_format",
                        "label": "Output format",
                        "desc": "**Output Format**:\n\n- **Text**: Replies in plain text format\n- **Markdown**: Uses Markdown format for replies\n- **JSON**: Uses JSON format for replies",
                        "type": 2,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "0"
                        },
                        "options": [
                            {
                                "label": "Text",
                                "value": "0"
                            },
                            {
                                "label": "Markdown",
                                "value": "1"
                            }
                        ],
                        "param_class": {
                            "class_id": 2,
                            "label": "Input and output settings"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "Current time",
                        "desc": "The current accurate time will be appended to each user query after enabled. [GuideDoc](https://www.coze.com/open/docs/guides/llm#3a97d6f3)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "Default instruction"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "Prompt leakage prevention",
                        "desc": "The system prompt will be reinforced after enabled, which can significantly reduce the probability of system prompt leakage. [GuideDoc](https://www.coze.com/open/docs/guides/llm#3a97d6f3)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "Default instruction"
                        }
                    }
                ]
            },
            "gemini-2.0-flash-001": {
                "display_info": {
                    "name": "Gemini 2.0 Flash",
                    "description": {
                        "zh_cn": "A versatile AI model for text, images, audio, and video",
                        "en_us": "A versatile AI model for text, images, audio, and video"
                    },
                    "output_tokens": 4096,
                    "max_tokens": 204800
                },
                "capability": {
                    "function_call": true,
                    "image_understanding": true,
                    "video_understanding": true,
                    "audio_understanding": true,
                    "support_multi_modal": true
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "Temperature",
                        "desc": "**Temperature**:\n\n- When you increase this value, the model outputs more diverse and innovative content; when you decrease it, the model outputs less diverse content that strictly follows the given instructions.\n- It is recommended not to adjust this value with \"Top p\" at the same time.",
                        "type": 1,
                        "min": "0",
                        "max": "2",
                        "precision": 2,
                        "default_val": {
                            "default_val": "1",
                            "creative": "0.8",
                            "balance": "0.5",
                            "precise": "0.1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "top_p",
                        "label": "Top p",
                        "desc": "**Top P**:\n\n- An alternative to sampling with temperature, where only tokens within the top p probability mass are considered. For example, 0.1 means only the top 10% probability mass tokens are considered.\n- We recommend altering this or temperature, but not both.",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0.94",
                            "creative": "1",
                            "balance": "1",
                            "precise": "1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "Response max length",
                        "desc": "You can specify the maximum length of the tokens output through this value. Typically, 100 tokens are approximately equal to 150 Chinese characters.",
                        "type": 2,
                        "min": "1",
                        "max": "8192",
                        "precision": 0,
                        "default_val": {
                            "default_val": "1024"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "Input and output settings"
                        }
                    },
                    {
                        "name": "response_format",
                        "label": "Output format",
                        "desc": "**Output Format**:\n\n- **Text**: Replies in plain text format\n- **Markdown**: Uses Markdown format for replies\n- **JSON**: Uses JSON format for replies",
                        "type": 2,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "0"
                        },
                        "options": [
                            {
                                "label": "Text",
                                "value": "0"
                            },
                            {
                                "label": "Markdown",
                                "value": "1"
                            }
                        ],
                        "param_class": {
                            "class_id": 2,
                            "label": "Input and output settings"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "Current time",
                        "desc": "The current accurate time will be appended to each user query after enabled. [GuideDoc](https://www.coze.com/open/docs/guides/llm#3a97d6f3)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "Default instruction"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "Prompt leakage prevention",
                        "desc": "The system prompt will be reinforced after enabled, which can significantly reduce the probability of system prompt leakage. [GuideDoc](https://www.coze.com/open/docs/guides/llm#3a97d6f3)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "Default instruction"
                        }
                    }
                ]
            },
            "gemini-2.5-pro-preview-05-06": {
                "display_info": {
                    "name": "Gemini 2.5 Pro",
                    "description": {
                        "zh_cn": "An advanced Gemini model with up to 1 million tokens",
                        "en_us": "An advanced Gemini model with up to 1 million tokens"
                    },
                    "output_tokens": 4096,
                    "max_tokens": 204800
                },
                "capability": {
                    "cot_display": true,
                    "function_call": true,
                    "image_understanding": true,
                    "video_understanding": true,
                    "audio_understanding": true,
                    "support_multi_modal": true
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "Temperature",
                        "desc": "**Temperature**:\n\n- When you increase this value, the model outputs more diverse and innovative content; when you decrease it, the model outputs less diverse content that strictly follows the given instructions.\n- It is recommended not to adjust this value with \"Top p\" at the same time.",
                        "type": 1,
                        "min": "0",
                        "max": "2",
                        "precision": 2,
                        "default_val": {
                            "default_val": "1",
                            "creative": "0.8",
                            "balance": "0.5",
                            "precise": "0.1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "top_p",
                        "label": "Top p",
                        "desc": "**Top P**:\n\n- An alternative to sampling with temperature, where only tokens within the top p probability mass are considered. For example, 0.1 means only the top 10% probability mass tokens are considered.\n- We recommend altering this or temperature, but not both.",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0.94",
                            "creative": "1",
                            "balance": "1",
                            "precise": "1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "Response max length",
                        "desc": "You can specify the maximum length of the tokens output through this value. Typically, 100 tokens are approximately equal to 150 Chinese characters.",
                        "type": 2,
                        "min": "5",
                        "max": "8192",
                        "precision": 0,
                        "default_val": {
                            "default_val": "1024"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "Input and output settings"
                        }
                    },
                    {
                        "name": "response_format",
                        "label": "Output format",
                        "desc": "**Output Format**:\n\n- **Text**: Replies in plain text format\n- **Markdown**: Uses Markdown format for replies\n- **JSON**: Uses JSON format for replies",
                        "type": 2,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "0"
                        },
                        "options": [
                            {
                                "label": "Text",
                                "value": "0"
                            },
                            {
                                "label": "Markdown",
                                "value": "1"
                            }
                        ],
                        "param_class": {
                            "class_id": 2,
                            "label": "Input and output settings"
                        }
                    },
                    {
                        "name": "thinking_type",
                        "label": "Thinking",
                        "desc": "After enabling deep thinking, before outputting the final answer, the model will first generate a segment of thought chain content to enhance the accuracy of the final response.",
                        "type": 4,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "disabled"
                        },
                        "options": [
                            {
                                "label": "enabled",
                                "value": "enabled"
                            },
                            {
                                "label": "disabled",
                                "value": "disabled"
                            }
                        ],
                        "param_class": {
                            "class_id": 6,
                            "label": "Deep Thinking"
                        }
                    },
                    {
                        "name": "thinking_budget_tokens",
                        "label": "Thinking Budget Tokens",
                        "desc": "Adjusting the output length of model's thinking result.",
                        "type": 2,
                        "min": "128",
                        "max": "8192",
                        "precision": 0,
                        "default_val": {
                            "default_val": "1024"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 6,
                            "label": "Deep Thinking"
                        }
                    }
                ]
            }
        },
        "QWen": {
            "default": {
                "display_info": {
                    "output_tokens": 4096,
                    "max_tokens": 8192
                },
                "capability": {
                    "function_call": true,
                    "image_understanding": false,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": false
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "生成随机性",
                        "desc": "- **temperature**: 调高温度会使得模型的输出更多样性和创新性，反之，降低温度会使输出内容更加遵循指令要求但减少多样性。建议不要与“Top p”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "1.99",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0.85",
                            "creative": "0.95",
                            "balance": "0.85",
                            "precise": "0.1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "top_p",
                        "label": "Top P",
                        "desc": "- **Top p 为累计概率**: 模型在生成输出时会从概率最高的词汇开始选择，直到这些词汇的总概率累积达到Top p 值。这样可以限制模型只选择这些高概率的词汇，从而控制输出内容的多样性。建议不要与“生成随机性”同时调整。",
                        "type": 1,
                        "min": "0.01",
                        "max": "1",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0.8",
                            "creative": "0.8",
                            "balance": "0.8",
                            "precise": "0.8"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "最大回复长度",
                        "desc": "控制模型输出的Tokens 长度上限。通常 100 Tokens 约等于 150 个中文汉字。",
                        "type": 2,
                        "min": "5",
                        "max": "2000",
                        "precision": 0,
                        "default_val": {
                            "default_val": "2000"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    },
                    {
                        "name": "response_format",
                        "label": "输出格式",
                        "desc": "- **文本**: 使用普通文本格式回复\n- **Markdown**: 将引导模型使用Markdown格式输出回复\n- **JSON**: 将引导模型使用JSON格式输出",
                        "type": 2,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "0"
                        },
                        "options": [
                            {
                                "label": "文本",
                                "value": "0"
                            },
                            {
                                "label": "Markdown",
                                "value": "1"
                            }
                        ],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    }
                ]
            }
        },
        "SEED": {
            "default": {
                "display_info": {
                    "output_tokens": 4096,
                    "max_tokens": 229376
                },
                "capability": {
                    "cot_display": true,
                    "function_call": true,
                    "image_understanding": true,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": true
                },
                "parameters": [
                    {
                        "name": "top_p",
                        "label": "Top P",
                        "desc": "- **Top p 为累计概率**: 模型在生成输出时会从概率最高的词汇开始选择，直到这些词汇的总概率累积达到Top p 值。这样可以限制模型只选择这些高概率的词汇，从而控制输出内容的多样性。建议不要与“生成随机性”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0.7",
                            "creative": "1",
                            "balance": "1",
                            "precise": "1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "frequency_penalty",
                        "label": "重复语句惩罚",
                        "desc": "- **frequency penalty**: 当该值为正时，会阻止模型频繁使用相同的词汇和短语，从而增加输出内容的多样性。",
                        "type": 1,
                        "min": "-2",
                        "max": "2",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0",
                            "creative": "0",
                            "balance": "0",
                            "precise": "0"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "最大回复长度",
                        "desc": "控制模型输出的Tokens 长度上限。通常 100 Tokens 约等于 150 个中文汉字。",
                        "type": 2,
                        "min": "0",
                        "max": "32768",
                        "precision": 0,
                        "default_val": {
                            "default_val": "4096"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    },
                    {
                        "name": "max_completion_tokens",
                        "label": "最大推理&回答长度",
                        "desc": "控制模型思维链推理和回复输出的最大长度（单位 token）。配置了该参数后，可以让模型输出超长内容，max_tokens （最大回复长度，默认值 4k）与思维链最大长度将失效，模型按需输出内容，直到达到“最大推理&回复长度”（max_completion_tokens） 配置的值。\n注意：若与“最大回复长度”（max_tokens） 字段同时设置，则“最大回复长度”不会生效。",
                        "type": 2,
                        "min": "0",
                        "max": "65536",
                        "precision": 0,
                        "default_val": {
                            "default_val": "0"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    }
                ]
            },
            "doubao-lite-32k-240828": {
                "display_info": {
                    "name": "豆包·通用模型·Lite",
                    "description": {
                        "zh_cn": "Doubao-lite-32k/240828，响应速度更快。",
                        "en_us": "Doubao-lite-32k/240828, faster response speed."
                    },
                    "output_tokens": 4096,
                    "max_tokens": 32768
                },
                "capability": {
                    "function_call": false,
                    "image_understanding": false,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": false
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "生成随机性",
                        "desc": "- **temperature**: 调高温度会使得模型的输出更多样性和创新性，反之，降低温度会使输出内容更加遵循指令要求但减少多样性。建议不要与“Top p”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 1,
                        "default_val": {
                            "default_val": "1",
                            "creative": "1",
                            "balance": "0.8",
                            "precise": "0.3"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "最大回复长度",
                        "desc": "控制模型输出的Tokens 长度上限。通常 100 Tokens 约等于 150 个中文汉字。",
                        "type": 2,
                        "min": "1",
                        "max": "4096",
                        "precision": 0,
                        "default_val": {
                            "default_val": "4096"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "当前时间",
                        "desc": "开启后，会在用户的每次query中拼上当前准确时间。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "SP防泄漏指令",
                        "desc": "开启后将会加固提示词，显著降低提示词泄露情况的出现概率。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    }
                ]
            },
            "doubao-pro-32k-241215": {
                "display_info": {
                    "name": "豆包·工具调用",
                    "description": {
                        "zh_cn": "Doubao-pro-32k/241215，主力模型，适合处理复杂任务，在参考问答、总结摘要、创作、文本分类、角色扮演等场景都有很好的效果。支持32k上下文窗口的推理和精调。",
                        "en_us": "Doubao-pro-32k/241215, the main model, suitable for handling complex tasks, with good performance in reference Q&A, summary, writing, text classification, and role-playing scenarios. It supports inference and fine-tuning with a 32k context window."
                    },
                    "output_tokens": 4096,
                    "max_tokens": 32768
                },
                "capability": {
                    "function_call": true,
                    "image_understanding": false,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": false
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "生成随机性",
                        "desc": "- **temperature**: 调高温度会使得模型的输出更多样性和创新性，反之，降低温度会使输出内容更加遵循指令要求但减少多样性。建议不要与“Top p”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 2,
                        "default_val": {
                            "default_val": "1",
                            "creative": "1",
                            "balance": "1",
                            "precise": "0.1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "top_p",
                        "label": "Top P",
                        "desc": "- **Top p 为累计概率**: 模型在生成输出时会从概率最高的词汇开始选择，直到这些词汇的总概率累积达到Top p 值。这样可以限制模型只选择这些高概率的词汇，从而控制输出内容的多样性。建议不要与“生成随机性”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0.7",
                            "creative": "0.8",
                            "balance": "0.7",
                            "precise": "0.7"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "response_format",
                        "label": "输出格式",
                        "desc": "- **文本**: 使用普通文本格式回复\n- **Markdown**: 将引导模型使用Markdown格式输出回复\n- **JSON**: 将引导模型使用JSON格式输出",
                        "type": 2,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "0"
                        },
                        "options": [
                            {
                                "label": "文本",
                                "value": "0"
                            },
                            {
                                "label": "Markdown",
                                "value": "1"
                            }
                        ],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "最大回复长度",
                        "desc": "控制模型输出的Tokens 长度上限。通常 100 Tokens 约等于 150 个中文汉字。",
                        "type": 2,
                        "min": "5",
                        "max": "4096",
                        "precision": 0,
                        "default_val": {
                            "default_val": "1024"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "当前时间",
                        "desc": "开启后，会在用户的每次query中拼上当前准确时间。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "SP防泄漏指令",
                        "desc": "开启后将会加固提示词，显著降低提示词泄露情况的出现概率。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    },
                    {
                        "name": "prefix_cache",
                        "label": "前缀缓存",
                        "desc": "使用前缀缓存可以提高模型应用的效率并降低成本。[指引文档](http://coze.cn/open/docs/guides/llm#8b3b9036)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 4,
                            "label": "上下文缓存"
                        }
                    }
                ]
            },
            "doubao-lite-32k-character-250228": {
                "display_info": {
                    "name": "豆包·角色扮演·Pro",
                    "description": {
                        "zh_cn": "Doubao-pro-32k/character-241215，角色扮演效果更优。",
                        "en_us": "Doubao-pro-32k/character-241215, better role-playing performance."
                    },
                    "output_tokens": 4096,
                    "max_tokens": 32768
                },
                "capability": {
                    "function_call": false,
                    "image_understanding": false,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": false
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "生成随机性",
                        "desc": "- **temperature**: 调高温度会使得模型的输出更多样性和创新性，反之，降低温度会使输出内容更加遵循指令要求但减少多样性。建议不要与“Top p”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 1,
                        "default_val": {
                            "default_val": "1",
                            "creative": "1",
                            "balance": "0.8",
                            "precise": "0.3"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "最大回复长度",
                        "desc": "控制模型输出的Tokens 长度上限。通常 100 Tokens 约等于 150 个中文汉字。",
                        "type": 2,
                        "min": "1",
                        "max": "4096",
                        "precision": 0,
                        "default_val": {
                            "default_val": "4096"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "当前时间",
                        "desc": "开启后，会在用户的每次query中拼上当前准确时间。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "SP防泄漏指令",
                        "desc": "开启后将会加固提示词，显著降低提示词泄露情况的出现概率。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    }
                ]
            },
            "doubao-1-5-pro-32k-250115": {
                "display_info": {
                    "name": "豆包·1.5·Pro·32k",
                    "description": {
                        "zh_cn": "Doubao-1.5-pro-32k-250115，全新一代主力模型，性能全面升级，在知识、代码、推理、等方面表现卓越。支持32k上下文窗口，输出长度支持最大12k tokens。",
                        "en_us": "Doubao-1.5-pro-32k-250115, the main model, with comprehensive performance upgrade, excelling in knowledge, code, reasoning, etc. It supports a 32k context window and maximum output length of 12k tokens."
                    },
                    "output_tokens": 4096,
                    "max_tokens": 32768
                },
                "capability": {
                    "function_call": true,
                    "image_understanding": false,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": false,
                    "prefill_resp": true
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "生成随机性",
                        "desc": "- **temperature**: 调高温度会使得模型的输出更多样性和创新性，反之，降低温度会使输出内容更加遵循指令要求但减少多样性。建议不要与“Top p”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 1,
                        "default_val": {
                            "default_val": "1",
                            "creative": "1",
                            "balance": "0.8",
                            "precise": "0.3"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "最大回复长度",
                        "desc": "控制模型输出的Tokens 长度上限。通常 100 Tokens 约等于 150 个中文汉字。",
                        "type": 2,
                        "min": "1",
                        "max": "12288",
                        "precision": 0,
                        "default_val": {
                            "default_val": "4096"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "当前时间",
                        "desc": "开启后，会在用户的每次query中拼上当前准确时间。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "SP防泄漏指令",
                        "desc": "开启后将会加固提示词，显著降低提示词泄露情况的出现概率。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    },
                    {
                        "name": "prefix_cache",
                        "label": "前缀缓存",
                        "desc": "使用前缀缓存可以提高模型应用的效率并降低成本。[指引文档](http://coze.cn/open/docs/guides/llm#8b3b9036)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 4,
                            "label": "上下文缓存"
                        }
                    }
                ]
            },
            "doubao-1-5-pro-256k-250115": {
                "display_info": {
                    "name": "豆包·1.5·Pro·256k",
                    "description": {
                        "zh_cn": "Doubao-1.5-pro-256k 基于 Doubao-1.5-Pro 全面升级版，整体效果大幅提升 10%。支持 256k 上下文窗口的推理，输出长度支持最大 12k tokens。更高性能、更大窗口、超高性价比，适用于更广泛的应用场景。",
                        "en_us": "Doubao-1.5-pro-256k, a comprehensive upgrade of Doubao-1.5-Pro, with overall performance improvement of 10%. It supports a 256k context window and maximum output length of 12k tokens. Higher performance, larger window, and higher cost-effectiveness, suitable for a wider range of applications."
                    },
                    "output_tokens": 4096,
                    "max_tokens": 262144
                },
                "capability": {
                    "function_call": true,
                    "image_understanding": false,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": false,
                    "prefill_resp": true
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "生成随机性",
                        "desc": "- **temperature**: 调高温度会使得模型的输出更多样性和创新性，反之，降低温度会使输出内容更加遵循指令要求但减少多样性。建议不要与“Top p”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 1,
                        "default_val": {
                            "default_val": "1",
                            "creative": "1",
                            "balance": "0.8",
                            "precise": "0.3"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "最大回复长度",
                        "desc": "控制模型输出的Tokens 长度上限。通常 100 Tokens 约等于 150 个中文汉字。",
                        "type": 2,
                        "min": "1",
                        "max": "12288",
                        "precision": 0,
                        "default_val": {
                            "default_val": "4096"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "当前时间",
                        "desc": "开启后，会在用户的每次query中拼上当前准确时间。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "SP防泄漏指令",
                        "desc": "开启后将会加固提示词，显著降低提示词泄露情况的出现概率。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    }
                ]
            },
            "doubao-1-5-lite-32k-250115": {
                "display_info": {
                    "name": "豆包·1.5·Lite·32k",
                    "description": {
                        "zh_cn": "Doubao-1.5-lite-32k/250115，全新一代轻量版模型，极致响应速度，效果与时延均达到全球一流水平。支持 32k 上下文窗口，输出长度支持最大 12k tokens。",
                        "en_us": "Doubao-1.5-lite-32k/250115, the light version of Doubao-1.5-Pro, with the highest response speed and performance. It supports a 32k context window and maximum output length of 12k tokens."
                    },
                    "output_tokens": 4096,
                    "max_tokens": 32768
                },
                "capability": {
                    "function_call": true,
                    "image_understanding": false,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": false
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "生成随机性",
                        "desc": "- **temperature**: 调高温度会使得模型的输出更多样性和创新性，反之，降低温度会使输出内容更加遵循指令要求但减少多样性。建议不要与“Top p”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 1,
                        "default_val": {
                            "default_val": "1",
                            "creative": "1",
                            "balance": "0.8",
                            "precise": "0.3"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "最大回复长度",
                        "desc": "控制模型输出的Tokens 长度上限。通常 100 Tokens 约等于 150 个中文汉字。",
                        "type": 2,
                        "min": "1",
                        "max": "4096",
                        "precision": 0,
                        "default_val": {
                            "default_val": "4096"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "当前时间",
                        "desc": "开启后，会在用户的每次query中拼上当前准确时间。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "SP防泄漏指令",
                        "desc": "开启后将会加固提示词，显著降低提示词泄露情况的出现概率。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    },
                    {
                        "name": "prefix_cache",
                        "label": "前缀缓存",
                        "desc": "使用前缀缓存可以提高模型应用的效率并降低成本。[指引文档](http://coze.cn/open/docs/guides/llm#8b3b9036)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 4,
                            "label": "上下文缓存"
                        }
                    }
                ]
            },
            "doubao-1-5-vision-pro-32k-250115": {
                "display_info": {
                    "name": "豆包·1.5·Pro·视觉理解",
                    "description": {
                        "zh_cn": "Doubao-1.5-pro-vision-32k/250115，具备强大的图片理解与推理能力，以及精准的指令理解能力。模型在图像文本信息抽取、基于图像的推理任务上有展现出了强大的性能，能够应用于更复杂、更广泛的视觉问答任务。",
                        "en_us": "Doubao-1.5-pro-vision-32k/250115, with powerful image understanding and reasoning capabilities, as well as precise instruction understanding. It has shown strong performance in image-text information extraction and image-based reasoning tasks, and can be applied to more complex and diverse visual question-answering tasks."
                    },
                    "output_tokens": 4096,
                    "max_tokens": 32768
                },
                "capability": {
                    "function_call": true,
                    "image_understanding": true,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": true
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "生成随机性",
                        "desc": "- **temperature**: 调高温度会使得模型的输出更多样性和创新性，反之，降低温度会使输出内容更加遵循指令要求但减少多样性。建议不要与“Top p”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 2,
                        "default_val": {
                            "default_val": "1",
                            "creative": "1",
                            "balance": "0.8",
                            "precise": "0.3"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "top_p",
                        "label": "Top P",
                        "desc": "- **Top p 为累计概率**: 模型在生成输出时会从概率最高的词汇开始选择，直到这些词汇的总概率累积达到Top p 值。这样可以限制模型只选择这些高概率的词汇，从而控制输出内容的多样性。建议不要与“生成随机性”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0.7",
                            "creative": "1",
                            "balance": "1",
                            "precise": "1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "frequency_penalty",
                        "label": "重复语句惩罚",
                        "desc": "- **frequency penalty**: 当该值为正时，会阻止模型频繁使用相同的词汇和短语，从而增加输出内容的多样性。",
                        "type": 1,
                        "min": "-2",
                        "max": "2",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "最大回复长度",
                        "desc": "控制模型输出的Tokens 长度上限。通常 100 Tokens 约等于 150 个中文汉字。",
                        "type": 2,
                        "min": "0",
                        "max": "4096",
                        "precision": 0,
                        "default_val": {
                            "default_val": "4096"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "当前时间",
                        "desc": "开启后，会在用户的每次query中拼上当前准确时间。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "SP防泄漏指令",
                        "desc": "开启后将会加固提示词，显著降低提示词泄露情况的出现概率。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    }
                ]
            },
            "doubao-1-5-thinking-pro-250415": {
                "display_info": {
                    "name": "豆包·1.5·Pro·深度思考·128K",
                    "description": {
                        "zh_cn": "Doubao-1.5-thinking-pro/250415，仅支持文本输入。在数学、编程、科学推理等专业领域及创意写作等通用任务中表现突出，在AIME 2024、Codeforces、GPQA等多项权威基准上达到或接近业界第一梯队水平。",
                        "en_us": "Doubao-1.5-thinking-pro/250415, only supports text input. It excels in mathematical, programming, scientific reasoning, and creative writing tasks. It has achieved or closely approached the industry-first level on multiple authoritative benchmarks such as AIME 2024, Codeforces, GPQA, etc."
                    },
                    "output_tokens": 4096,
                    "max_tokens": 131072
                },
                "capability": {
                    "cot_display": true,
                    "function_call": true,
                    "image_understanding": false,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": false
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "生成随机性",
                        "desc": "- **temperature**: 调高温度会使得模型的输出更多样性和创新性，反之，降低温度会使输出内容更加遵循指令要求但减少多样性。建议不要与“Top p”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "2",
                        "precision": 2,
                        "default_val": {
                            "default_val": "1",
                            "creative": "1",
                            "balance": "0.8",
                            "precise": "0.3"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "top_p",
                        "label": "Top P",
                        "desc": "- **Top p 为累计概率**: 模型在生成输出时会从概率最高的词汇开始选择，直到这些词汇的总概率累积达到Top p 值。这样可以限制模型只选择这些高概率的词汇，从而控制输出内容的多样性。建议不要与“生成随机性”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0.7"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "frequency_penalty",
                        "label": "重复语句惩罚",
                        "desc": "- **frequency penalty**: 当该值为正时，会阻止模型频繁使用相同的词汇和短语，从而增加输出内容的多样性。",
                        "type": 1,
                        "min": "-2",
                        "max": "2",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "最大回复长度",
                        "desc": "控制模型输出的Tokens 长度上限。通常 100 Tokens 约等于 150 个中文汉字。",
                        "type": 2,
                        "min": "0",
                        "max": "16384",
                        "precision": 0,
                        "default_val": {
                            "default_val": "4096"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "当前时间",
                        "desc": "开启后，会在用户的每次query中拼上当前准确时间。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "SP防泄漏指令",
                        "desc": "开启后将会加固提示词，显著降低提示词泄露情况的出现概率。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    }
                ]
            },
            "doubao-1-5-thinking-pro-m-250428": {
                "display_info": {
                    "name": "豆包·1.5·Pro·视觉推理·128K",
                    "description": {
                        "zh_cn": "Doubao-1.5-thinking-pro/m-250415，基于深度思考+视觉理解的混合训练，让模型具备视觉推理能力，更强的多模态交互能力，和更低的视觉描述幻觉。",
                        "en_us": "Doubao-1.5-thinking-pro/m-250415, based on deep thinking and visual understanding, it enables the model to have visual reasoning capabilities, stronger multi-modal interaction abilities, and lower visual description hallucinations."
                    },
                    "output_tokens": 4096,
                    "max_tokens": 131072
                },
                "capability": {
                    "cot_display": true,
                    "function_call": true,
                    "image_understanding": true,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": true
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "生成随机性",
                        "desc": "- **temperature**: 调高温度会使得模型的输出更多样性和创新性，反之，降低温度会使输出内容更加遵循指令要求但减少多样性。建议不要与“Top p”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "2",
                        "precision": 2,
                        "default_val": {
                            "default_val": "1",
                            "creative": "1",
                            "balance": "0.8",
                            "precise": "0.3"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "top_p",
                        "label": "Top P",
                        "desc": "- **Top p 为累计概率**: 模型在生成输出时会从概率最高的词汇开始选择，直到这些词汇的总概率累积达到Top p 值。这样可以限制模型只选择这些高概率的词汇，从而控制输出内容的多样性。建议不要与“生成随机性”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0.7"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "frequency_penalty",
                        "label": "重复语句惩罚",
                        "desc": "- **frequency penalty**: 当该值为正时，会阻止模型频繁使用相同的词汇和短语，从而增加输出内容的多样性。",
                        "type": 1,
                        "min": "-2",
                        "max": "2",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "最大回复长度",
                        "desc": "控制模型输出的Tokens 长度上限。通常 100 Tokens 约等于 150 个中文汉字。",
                        "type": 2,
                        "min": "0",
                        "max": "16384",
                        "precision": 0,
                        "default_val": {
                            "default_val": "4096"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "当前时间",
                        "desc": "开启后，会在用户的每次query中拼上当前准确时间。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "SP防泄漏指令",
                        "desc": "开启后将会加固提示词，显著降低提示词泄露情况的出现概率。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    }
                ]
            },
            "doubao-1-5-thinking-vision-pro-250428": {
                "display_info": {
                    "name": "豆包·1.5·Pro·视觉深度思考",
                    "description": {
                        "zh_cn": "doubao-1-5-thinking-vision-pro-250428，最新发布的视觉-语言多模态大模型，具备更强的通用多模态理解和推理能力，在 59 个公开评测基准中的 37 个上取得 SOTA 表现。 ",
                        "en_us": "Doubao-1.5-thinking-vision-pro-250428, the latest visual-language multimodal large model, with stronger general multimodal understanding and reasoning capabilities. It has achieved SOTA performance on 37 out of 59 publicly available benchmarks."
                    },
                    "output_tokens": 4096,
                    "max_tokens": 131072
                },
                "capability": {
                    "cot_display": true,
                    "function_call": true,
                    "image_understanding": true,
                    "video_understanding": true,
                    "audio_understanding": false,
                    "support_multi_modal": true
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "生成随机性",
                        "desc": "- **temperature**: 调高温度会使得模型的输出更多样性和创新性，反之，降低温度会使输出内容更加遵循指令要求但减少多样性。建议不要与“Top p”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "2",
                        "precision": 1,
                        "default_val": {
                            "default_val": "1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "top_p",
                        "label": "Top P",
                        "desc": "- **Top p 为累计概率**: 模型在生成输出时会从概率最高的词汇开始选择，直到这些词汇的总概率累积达到Top p 值。这样可以限制模型只选择这些高概率的词汇，从而控制输出内容的多样性。建议不要与“生成随机性”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 1,
                        "default_val": {
                            "default_val": "0.7"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "最大回复长度",
                        "desc": "控制模型输出的Tokens 长度上限。通常 100 Tokens 约等于 150 个中文汉字。",
                        "type": 2,
                        "min": "1",
                        "max": "16384",
                        "precision": 0,
                        "default_val": {
                            "default_val": "4096"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    },
                    {
                        "name": "response_format",
                        "label": "输出格式",
                        "desc": "- **文本**: 使用普通文本格式回复\n- **Markdown**: 将引导模型使用Markdown格式输出回复\n- **JSON**: 将引导模型使用JSON格式输出",
                        "type": 2,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "0"
                        },
                        "options": [
                            {
                                "label": "文本",
                                "value": "0"
                            },
                            {
                                "label": "Markdown",
                                "value": "1"
                            },
                            {
                                "label": "JSON",
                                "value": "2"
                            }
                        ],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    },
                    {
                        "name": "thinking_type",
                        "label": "深度思考开关",
                        "desc": "开启深度思考后，在输出最终回答之前，模型会先输出一段思维链内容，以提升最终答案的准确性。",
                        "type": 4,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "enabled"
                        },
                        "options": [
                            {
                                "label": "开启",
                                "value": "enabled"
                            },
                            {
                                "label": "关闭",
                                "value": "disabled"
                            }
                        ],
                        "param_class": {
                            "class_id": 6,
                            "label": "深度思考"
                        }
                    }
                ]
            },
            "doubao-1.5-vision-pro-250328": {
                "display_info": {
                    "name": "豆包·1.5·Pro·视觉理解-250328",
                    "description": {
                        "zh_cn": "doubao-1.5-vision-pro-250328，全新升级的多模态大模型，视觉理解、分类、信息抽取等能力显著提升，并重点增强了解题、视频理解等场景的任务效果。支持 128k 上下文窗口，输出长度支持最大 16k tokens。  ",
                        "en_us": "Doubao-1.5-vision-pro-250328, the latest multimodal large model with enhanced visual understanding, classification, and information extraction capabilities. It has significantly improved task performance in solving problems, video understanding, and other scenarios. It supports a 128k context window and a maximum output length of 16k tokens."
                    },
                    "output_tokens": 4096,
                    "max_tokens": 131072
                },
                "capability": {
                    "function_call": false,
                    "image_understanding": true,
                    "video_understanding": true,
                    "audio_understanding": false,
                    "support_multi_modal": true
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "生成随机性",
                        "desc": "- **temperature**: 调高温度会使得模型的输出更多样性和创新性，反之，降低温度会使输出内容更加遵循指令要求但减少多样性。建议不要与“Top p”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "2",
                        "precision": 1,
                        "default_val": {
                            "default_val": "1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "frequency_penalty",
                        "label": "重复语句惩罚",
                        "desc": "- **frequency penalty**: 当该值为正时，会阻止模型频繁使用相同的词汇和短语，从而增加输出内容的多样性。",
                        "type": 1,
                        "min": "-2",
                        "max": "2",
                        "precision": 1,
                        "default_val": {
                            "default_val": "0"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "top_p",
                        "label": "Top P",
                        "desc": "- **Top p 为累计概率**: 模型在生成输出时会从概率最高的词汇开始选择，直到这些词汇的总概率累积达到Top p 值。这样可以限制模型只选择这些高概率的词汇，从而控制输出内容的多样性。建议不要与“生成随机性”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 1,
                        "default_val": {
                            "default_val": "0.7"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "最大回复长度",
                        "desc": "控制模型输出的Tokens 长度上限。通常 100 Tokens 约等于 150 个中文汉字。",
                        "type": 2,
                        "min": "1",
                        "max": "16384",
                        "precision": 0,
                        "default_val": {
                            "default_val": "4096"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    },
                    {
                        "name": "response_format",
                        "label": "输出格式",
                        "desc": "- **文本**: 使用普通文本格式回复\n- **Markdown**: 将引导模型使用Markdown格式输出回复\n- **JSON**: 将引导模型使用JSON格式输出",
                        "type": 2,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "0"
                        },
                        "options": [
                            {
                                "label": "文本",
                                "value": "0"
                            },
                            {
                                "label": "Markdown",
                                "value": "1"
                            }
                        ],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "当前时间",
                        "desc": "开启后，会在用户的每次query中拼上当前准确时间。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "SP防泄漏指令",
                        "desc": "开启后将会加固提示词，显著降低提示词泄露情况的出现概率。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    }
                ]
            },
            "doubao-1-5-pro-32k-character-250228": {
                "display_info": {
                    "name": "豆包·1.5·Pro·角色扮演",
                    "description": {
                        "zh_cn": "doubao-1-5-pro-32k-character-250228，基于Doubao-1.5全新升级，支持故事剧情模式，优化恋爱拉扯能力（GSB+11%），角色风格能力优化 ，增强剧情推动能力",
                        "en_us": "Doubao-1.5-pro-32k-character-250228, the latest version of Doubao-1.5 with enhanced storytelling and role-playing capabilities. It optimizes the ability to handle love affairs (GSB+11%) and improves the role-style ability. Additionally, it enhances the story-driven capabilities."
                    },
                    "output_tokens": 4096,
                    "max_tokens": 32768
                },
                "capability": {
                    "function_call": false,
                    "image_understanding": false,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": false
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "生成随机性",
                        "desc": "- **temperature**: 调高温度会使得模型的输出更多样性和创新性，反之，降低温度会使输出内容更加遵循指令要求但减少多样性。建议不要与“Top p”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 1,
                        "default_val": {
                            "default_val": "1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "frequency_penalty",
                        "label": "重复语句惩罚",
                        "desc": "- **frequency penalty**: 当该值为正时，会阻止模型频繁使用相同的词汇和短语，从而增加输出内容的多样性。",
                        "type": 1,
                        "min": "-2",
                        "max": "2",
                        "precision": 1,
                        "default_val": {
                            "default_val": "0"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "top_p",
                        "label": "Top P",
                        "desc": "- **Top p 为累计概率**: 模型在生成输出时会从概率最高的词汇开始选择，直到这些词汇的总概率累积达到Top p 值。这样可以限制模型只选择这些高概率的词汇，从而控制输出内容的多样性。建议不要与“生成随机性”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 1,
                        "default_val": {
                            "default_val": "0.7"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "最大回复长度",
                        "desc": "控制模型输出的Tokens 长度上限。通常 100 Tokens 约等于 150 个中文汉字。",
                        "type": 2,
                        "min": "1",
                        "max": "12288",
                        "precision": 0,
                        "default_val": {
                            "default_val": "4096"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    },
                    {
                        "name": "response_format",
                        "label": "输出格式",
                        "desc": "- **文本**: 使用普通文本格式回复\n- **Markdown**: 将引导模型使用Markdown格式输出回复\n- **JSON**: 将引导模型使用JSON格式输出",
                        "type": 2,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "0"
                        },
                        "options": [
                            {
                                "label": "文本",
                                "value": "0"
                            },
                            {
                                "label": "Markdown",
                                "value": "1"
                            },
                            {
                                "label": "JSON",
                                "value": "2"
                            }
                        ],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "当前时间",
                        "desc": "开启后，会在用户的每次query中拼上当前准确时间。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "SP防泄漏指令",
                        "desc": "开启后将会加固提示词，显著降低提示词泄露情况的出现概率。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    }
                ]
            },
            "doubao-1-5-ui-tars-250428": {
                "display_info": {
                    "name": "豆包·GUI·Agent模型",
                    "description": {
                        "zh_cn": "Doubao-1.5-UI-TARS 是一款原生面向图形界面交互（GUI）的Agent模型。通过感知、推理和行动等类人的能力，与 GUI 进行无缝交互。",
                        "en_us": "Doubao-1.5-UI-TARS is a native GUI Agent model that enables seamless interaction with GUI applications. It has the ability to perceive, reason, and act like a human, making it a powerful tool for automating tasks and enhancing user experiences."
                    },
                    "output_tokens": 4096,
                    "max_tokens": 32768
                },
                "capability": {
                    "cot_display": true,
                    "function_call": false,
                    "image_understanding": true,
                    "video_understanding": true,
                    "audio_understanding": false,
                    "support_multi_modal": true
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "生成随机性",
                        "desc": "- **temperature**: 调高温度会使得模型的输出更多样性和创新性，反之，降低温度会使输出内容更加遵循指令要求但减少多样性。建议不要与“Top p”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 1,
                        "default_val": {
                            "default_val": "1",
                            "creative": "0.8",
                            "balance": "0.5",
                            "precise": "0.2"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "frequency_penalty",
                        "label": "重复语句惩罚",
                        "desc": "- **frequency penalty**: 当该值为正时，会阻止模型频繁使用相同的词汇和短语，从而增加输出内容的多样性。",
                        "type": 1,
                        "min": "-2",
                        "max": "2",
                        "precision": 1,
                        "default_val": {
                            "default_val": "0",
                            "creative": "-2",
                            "balance": "0",
                            "precise": "2"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "top_p",
                        "label": "Top P",
                        "desc": "- **Top p 为累计概率**: 模型在生成输出时会从概率最高的词汇开始选择，直到这些词汇的总概率累积达到Top p 值。这样可以限制模型只选择这些高概率的词汇，从而控制输出内容的多样性。建议不要与“生成随机性”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 1,
                        "default_val": {
                            "default_val": "0.7",
                            "creative": "1",
                            "balance": "1",
                            "precise": "1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "最大回复长度",
                        "desc": "控制模型输出的Tokens 长度上限。通常 100 Tokens 约等于 150 个中文汉字。",
                        "type": 2,
                        "min": "1",
                        "max": "12288",
                        "precision": 0,
                        "default_val": {
                            "default_val": "4096"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    },
                    {
                        "name": "response_format",
                        "label": "输出格式",
                        "desc": "- **文本**: 使用普通文本格式回复\n- **Markdown**: 将引导模型使用Markdown格式输出回复\n- **JSON**: 将引导模型使用JSON格式输出",
                        "type": 2,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "0"
                        },
                        "options": [
                            {
                                "label": "文本",
                                "value": "0"
                            },
                            {
                                "label": "Markdown",
                                "value": "1"
                            },
                            {
                                "label": "JSON",
                                "value": "2"
                            }
                        ],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "当前时间",
                        "desc": "开启后，会在用户的每次query中拼上当前准确时间。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "SP防泄漏指令",
                        "desc": "开启后将会加固提示词，显著降低提示词泄露情况的出现概率。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    },
                    {
                        "name": "thinking_type",
                        "label": "深度思考开关",
                        "desc": "开启深度思考后，在输出最终回答之前，模型会先输出一段思维链内容，以提升最终答案的准确性。",
                        "type": 4,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "enabled"
                        },
                        "options": [
                            {
                                "label": "开启",
                                "value": "enabled"
                            },
                            {
                                "label": "关闭",
                                "value": "disabled"
                            }
                        ],
                        "param_class": {
                            "class_id": 6,
                            "label": "深度思考"
                        }
                    }
                ]
            },
            "doubao-seed-1-6-thinking-250615": {
                "display_info": {
                    "name": "豆包·1.6·深度思考",
                    "description": {
                        "zh_cn": "Doubao-1.6-thinking模型思考能力大幅强化， 对比Doubao-1.5-thinking-pro，在Coding、Math、 逻辑推理等基础能力上进一步提升， 支持视觉理解。 支持 256k 上下文窗口，输出长度支持最大 16k tokens。",
                        "en_us": "Doubao-1.6-thinking model significantly enhances its thinking capabilities compared to Doubao-1.5-thinking-pro. It further improves its base capabilities in areas such as Coding, Math, and logical reasoning. Additionally, it supports visual understanding. The model has a 256k context window and a maximum output length of 16k tokens."
                    },
                    "output_tokens": 4096,
                    "max_tokens": 229376
                },
                "capability": {
                    "cot_display": true,
                    "function_call": true,
                    "image_understanding": true,
                    "video_understanding": true,
                    "audio_understanding": false,
                    "support_multi_modal": true
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "生成随机性",
                        "desc": "- **temperature**: 调高温度会使得模型的输出更多样性和创新性，反之，降低温度会使输出内容更加遵循指令要求但减少多样性。建议不要与“Top p”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "2",
                        "precision": 2,
                        "default_val": {
                            "default_val": "1",
                            "creative": "1",
                            "balance": "0.8",
                            "precise": "0.3"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "top_p",
                        "label": "Top P",
                        "desc": "- **Top p 为累计概率**: 模型在生成输出时会从概率最高的词汇开始选择，直到这些词汇的总概率累积达到Top p 值。这样可以限制模型只选择这些高概率的词汇，从而控制输出内容的多样性。建议不要与“生成随机性”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0.7",
                            "creative": "1",
                            "balance": "1",
                            "precise": "1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "frequency_penalty",
                        "label": "重复语句惩罚",
                        "desc": "- **frequency penalty**: 当该值为正时，会阻止模型频繁使用相同的词汇和短语，从而增加输出内容的多样性。",
                        "type": 1,
                        "min": "-2",
                        "max": "2",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0",
                            "creative": "0",
                            "balance": "0",
                            "precise": "0"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "最大回复长度",
                        "desc": "控制模型输出的Tokens 长度上限。通常 100 Tokens 约等于 150 个中文汉字。",
                        "type": 2,
                        "min": "0",
                        "max": "16384",
                        "precision": 0,
                        "default_val": {
                            "default_val": "4096"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "当前时间",
                        "desc": "开启后，会在用户的每次query中拼上当前准确时间。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "SP防泄漏指令",
                        "desc": "开启后将会加固提示词，显著降低提示词泄露情况的出现概率。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    },
                    {
                        "name": "max_completion_tokens",
                        "label": "最大推理&回答长度",
                        "desc": "控制模型思维链推理和回复输出的最大长度（单位 token）。配置了该参数后，可以让模型输出超长内容，max_tokens （最大回复长度，默认值 4k）与思维链最大长度将失效，模型按需输出内容，直到达到“最大推理&回复长度”（max_completion_tokens） 配置的值。\n注意：若与“最大回复长度”（max_tokens） 字段同时设置，则“最大回复长度”不会生效。",
                        "type": 2,
                        "min": "0",
                        "max": "65536",
                        "precision": 0,
                        "default_val": {
                            "default_val": "0"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    }
                ]
            },
            "doubao-seed-1-6-251015": {
                "display_info": {
                    "name": "豆包·1.6·自动深度思考",
                    "description": {
                        "zh_cn": "Doubao-1.6，全新多模态深度思考模型，支持 256k 上下文窗口，输出长度支持最大 16k tokens。支持开启/关闭思考功能。",
                        "en_us": "Doubao-1.6, a new multimodal deep thinking model that supports a 256k context window and maximum output length of 16k tokens. It also supports enabling/disabling thinking mode."
                    },
                    "output_tokens": 4096,
                    "max_tokens": 262144
                },
                "capability": {
                    "cot_display": true,
                    "function_call": true,
                    "image_understanding": true,
                    "video_understanding": true,
                    "audio_understanding": false,
                    "support_multi_modal": true
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "生成随机性",
                        "desc": "- **temperature**: 调高温度会使得模型的输出更多样性和创新性，反之，降低温度会使输出内容更加遵循指令要求但减少多样性。建议不要与“Top p”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "2",
                        "precision": 2,
                        "default_val": {
                            "default_val": "1",
                            "creative": "1",
                            "balance": "0.8",
                            "precise": "0.3"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "top_p",
                        "label": "Top P",
                        "desc": "- **Top p 为累计概率**: 模型在生成输出时会从概率最高的词汇开始选择，直到这些词汇的总概率累积达到Top p 值。这样可以限制模型只选择这些高概率的词汇，从而控制输出内容的多样性。建议不要与“生成随机性”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0.7",
                            "creative": "1",
                            "balance": "1",
                            "precise": "1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "frequency_penalty",
                        "label": "重复语句惩罚",
                        "desc": "- **frequency penalty**: 当该值为正时，会阻止模型频繁使用相同的词汇和短语，从而增加输出内容的多样性。",
                        "type": 1,
                        "min": "-2",
                        "max": "2",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0",
                            "creative": "0",
                            "balance": "0",
                            "precise": "0"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "最大回复长度",
                        "desc": "控制模型输出的Tokens 长度上限。通常 100 Tokens 约等于 150 个中文汉字。",
                        "type": 2,
                        "min": "0",
                        "max": "32768",
                        "precision": 0,
                        "default_val": {
                            "default_val": "4096"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "当前时间",
                        "desc": "开启后，会在用户的每次query中拼上当前准确时间。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "SP防泄漏指令",
                        "desc": "开启后将会加固提示词，显著降低提示词泄露情况的出现概率。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    },
                    {
                        "name": "thinking_type",
                        "label": "深度思考开关",
                        "desc": "开启深度思考后，在输出最终回答之前，模型会先输出一段思维链内容，以提升最终答案的准确性。",
                        "type": 4,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "auto"
                        },
                        "options": [
                            {
                                "label": "开启",
                                "value": "enabled"
                            },
                            {
                                "label": "关闭",
                                "value": "disabled"
                            },
                            {
                                "label": "自动",
                                "value": "auto"
                            }
                        ],
                        "param_class": {
                            "class_id": 6,
                            "label": "深度思考"
                        }
                    },
                    {
                        "name": "max_completion_tokens",
                        "label": "最大推理&回答长度",
                        "desc": "控制模型思维链推理和回复输出的最大长度（单位 token）。配置了该参数后，可以让模型输出超长内容，max_tokens （最大回复长度，默认值 4k）与思维链最大长度将失效，模型按需输出内容，直到达到“最大推理&回复长度”（max_completion_tokens） 配置的值。\n注意：若与“最大回复长度”（max_tokens） 字段同时设置，则“最大回复长度”不会生效。",
                        "type": 2,
                        "min": "0",
                        "max": "65536",
                        "precision": 0,
                        "default_val": {
                            "default_val": "0"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    }
                ]
            },
            "doubao-seed-1-6-flash-250828": {
                "display_info": {
                    "name": "豆包·1.6·极致速度",
                    "description": {
                        "zh_cn": "Doubao-1-6-flash-250615，推理速度极致的多模态深度思考模型，TPOT仅需10ms； 同时支持文本和视觉理解，文本理解能力超过上一代lite，视觉理解比肩友商pro系列模型。支持 256k 上下文窗口，输出长度支持最大 16k tokens。",
                        "en_us": "Doubao-1-6-flash-250828, with the fastest inference speed among all Doubao models, it only takes 10ms to process a single query. It also supports both text and visual understanding, with text understanding capabilities exceeding those of the previous generation lite model. Its visual understanding capabilities are on par with those of friendlier pro series models. It supports a 256k context window and maximum output length of 16k tokens."
                    },
                    "output_tokens": 4096,
                    "max_tokens": 229376
                },
                "capability": {
                    "cot_display": true,
                    "function_call": true,
                    "image_understanding": true,
                    "video_understanding": true,
                    "audio_understanding": false,
                    "support_multi_modal": true
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "生成随机性",
                        "desc": "- **temperature**: 调高温度会使得模型的输出更多样性和创新性，反之，降低温度会使输出内容更加遵循指令要求但减少多样性。建议不要与“Top p”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "2",
                        "precision": 2,
                        "default_val": {
                            "default_val": "1",
                            "creative": "1",
                            "balance": "0.8",
                            "precise": "0.2"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "top_p",
                        "label": "Top P",
                        "desc": "- **Top p 为累计概率**: 模型在生成输出时会从概率最高的词汇开始选择，直到这些词汇的总概率累积达到Top p 值。这样可以限制模型只选择这些高概率的词汇，从而控制输出内容的多样性。建议不要与“生成随机性”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0.7",
                            "creative": "1",
                            "balance": "1",
                            "precise": "1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "frequency_penalty",
                        "label": "重复语句惩罚",
                        "desc": "- **frequency penalty**: 当该值为正时，会阻止模型频繁使用相同的词汇和短语，从而增加输出内容的多样性。",
                        "type": 1,
                        "min": "-2",
                        "max": "2",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0",
                            "creative": "0",
                            "balance": "0",
                            "precise": "0"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "最大回复长度",
                        "desc": "控制模型输出的Tokens 长度上限。通常 100 Tokens 约等于 150 个中文汉字。",
                        "type": 2,
                        "min": "0",
                        "max": "32768",
                        "precision": 0,
                        "default_val": {
                            "default_val": "4096"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "当前时间",
                        "desc": "开启后，会在用户的每次query中拼上当前准确时间。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "SP防泄漏指令",
                        "desc": "开启后将会加固提示词，显著降低提示词泄露情况的出现概率。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    },
                    {
                        "name": "thinking_type",
                        "label": "深度思考开关",
                        "desc": "开启深度思考后，在输出最终回答之前，模型会先输出一段思维链内容，以提升最终答案的准确性。",
                        "type": 4,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "enabled"
                        },
                        "options": [
                            {
                                "label": "开启",
                                "value": "enabled"
                            },
                            {
                                "label": "关闭",
                                "value": "disabled"
                            }
                        ],
                        "param_class": {
                            "class_id": 6,
                            "label": "深度思考"
                        }
                    },
                    {
                        "name": "max_completion_tokens",
                        "label": "最大推理&回答长度",
                        "desc": "控制模型思维链推理和回复输出的最大长度（单位 token）。配置了该参数后，可以让模型输出超长内容，max_tokens （最大回复长度，默认值 4k）与思维链最大长度将失效，模型按需输出内容，直到达到“最大推理&回复长度”（max_completion_tokens） 配置的值。\n注意：若与“最大回复长度”（max_tokens） 字段同时设置，则“最大回复长度”不会生效。",
                        "type": 2,
                        "min": "0",
                        "max": "65536",
                        "precision": 0,
                        "default_val": {
                            "default_val": "0"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    }
                ]
            },
            "doubao-seed-1-6-thinking-250715": {
                "display_info": {
                    "name": "豆包·1.6·深度思考·250715",
                    "description": {
                        "zh_cn": "Doubao-1.6-thinking-250715，深度思考能力更强化！相比250615版本文本&视觉能力显著提升，综合能力领先Doubao-Seed-1.6-250615开启thinking模式。",
                        "en_us": "Doubao-1.6-thinking-250715, with enhanced deep thinking capabilities, it significantly improves the performance of text and visual tasks compared to the 250615 version. Its overall performance is leading Doubao-Seed-1.6-250615 when enabled in thinking mode."
                    },
                    "output_tokens": 4096,
                    "max_tokens": 229376
                },
                "capability": {
                    "cot_display": true,
                    "function_call": true,
                    "image_understanding": true,
                    "video_understanding": true,
                    "audio_understanding": false,
                    "support_multi_modal": true
                },
                "parameters": [
                    {
                        "name": "top_p",
                        "label": "Top P",
                        "desc": "- **Top p 为累计概率**: 模型在生成输出时会从概率最高的词汇开始选择，直到这些词汇的总概率累积达到Top p 值。这样可以限制模型只选择这些高概率的词汇，从而控制输出内容的多样性。建议不要与“生成随机性”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0.7",
                            "creative": "1",
                            "balance": "1",
                            "precise": "1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "frequency_penalty",
                        "label": "重复语句惩罚",
                        "desc": "- **frequency penalty**: 当该值为正时，会阻止模型频繁使用相同的词汇和短语，从而增加输出内容的多样性。",
                        "type": 1,
                        "min": "-2",
                        "max": "2",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0",
                            "creative": "0",
                            "balance": "0",
                            "precise": "0"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "最大回复长度",
                        "desc": "控制模型输出的Tokens 长度上限。通常 100 Tokens 约等于 150 个中文汉字。",
                        "type": 2,
                        "min": "0",
                        "max": "32768",
                        "precision": 0,
                        "default_val": {
                            "default_val": "4096"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "当前时间",
                        "desc": "开启后，会在用户的每次query中拼上当前准确时间。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "SP防泄漏指令",
                        "desc": "开启后将会加固提示词，显著降低提示词泄露情况的出现概率。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    },
                    {
                        "name": "max_completion_tokens",
                        "label": "最大推理&回答长度",
                        "desc": "控制模型思维链推理和回复输出的最大长度（单位 token）。配置了该参数后，可以让模型输出超长内容，max_tokens （最大回复长度，默认值 4k）与思维链最大长度将失效，模型按需输出内容，直到达到“最大推理&回复长度”（max_completion_tokens） 配置的值。\n注意：若与“最大回复长度”（max_tokens） 字段同时设置，则“最大回复长度”不会生效。",
                        "type": 2,
                        "min": "0",
                        "max": "65536",
                        "precision": 0,
                        "default_val": {
                            "default_val": "0"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    }
                ]
            },
            "doubao-seed-1-6-flash-250715": {
                "display_info": {
                    "name": "豆包·1.6·极致速度·250715",
                    "description": {
                        "zh_cn": "Doubao-1.6-flash-250715，相比flash-0615版本，0715版本思考与非思考模式的纯文本任务效果大幅提升近10%。",
                        "en_us": "Doubao-1.6-flash-250715, compared to the flash-0615 version, the 0715 version has a 10% improvement in the performance of pure text tasks in both thinking and non-thinking modes."
                    },
                    "output_tokens": 4096,
                    "max_tokens": 229376
                },
                "capability": {
                    "cot_display": true,
                    "function_call": true,
                    "image_understanding": true,
                    "video_understanding": true,
                    "audio_understanding": false,
                    "support_multi_modal": true
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "生成随机性",
                        "desc": "- **temperature**: 调高温度会使得模型的输出更多样性和创新性，反之，降低温度会使输出内容更加遵循指令要求但减少多样性。建议不要与“Top p”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "2",
                        "precision": 2,
                        "default_val": {
                            "default_val": "1",
                            "creative": "1",
                            "balance": "0.8",
                            "precise": "0.2"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "top_p",
                        "label": "Top P",
                        "desc": "- **Top p 为累计概率**: 模型在生成输出时会从概率最高的词汇开始选择，直到这些词汇的总概率累积达到Top p 值。这样可以限制模型只选择这些高概率的词汇，从而控制输出内容的多样性。建议不要与“生成随机性”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0.7",
                            "creative": "1",
                            "balance": "1",
                            "precise": "1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "frequency_penalty",
                        "label": "重复语句惩罚",
                        "desc": "- **frequency penalty**: 当该值为正时，会阻止模型频繁使用相同的词汇和短语，从而增加输出内容的多样性。",
                        "type": 1,
                        "min": "-2",
                        "max": "2",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0",
                            "creative": "0",
                            "balance": "0",
                            "precise": "0"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "最大回复长度",
                        "desc": "控制模型输出的Tokens 长度上限。通常 100 Tokens 约等于 150 个中文汉字。",
                        "type": 2,
                        "min": "0",
                        "max": "32768",
                        "precision": 0,
                        "default_val": {
                            "default_val": "4096"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "当前时间",
                        "desc": "开启后，会在用户的每次query中拼上当前准确时间。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "SP防泄漏指令",
                        "desc": "开启后将会加固提示词，显著降低提示词泄露情况的出现概率。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    },
                    {
                        "name": "thinking_type",
                        "label": "深度思考开关",
                        "desc": "开启深度思考后，在输出最终回答之前，模型会先输出一段思维链内容，以提升最终答案的准确性。",
                        "type": 4,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "enabled"
                        },
                        "options": [
                            {
                                "label": "开启",
                                "value": "enabled"
                            },
                            {
                                "label": "关闭",
                                "value": "disabled"
                            }
                        ],
                        "param_class": {
                            "class_id": 6,
                            "label": "深度思考"
                        }
                    },
                    {
                        "name": "max_completion_tokens",
                        "label": "最大推理&回答长度",
                        "desc": "控制模型思维链推理和回复输出的最大长度（单位 token）。配置了该参数后，可以让模型输出超长内容，max_tokens （最大回复长度，默认值 4k）与思维链最大长度将失效，模型按需输出内容，直到达到“最大推理&回复长度”（max_completion_tokens） 配置的值。\n注意：若与“最大回复长度”（max_tokens） 字段同时设置，则“最大回复长度”不会生效。",
                        "type": 2,
                        "min": "0",
                        "max": "65536",
                        "precision": 0,
                        "default_val": {
                            "default_val": "0"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    }
                ]
            },
            "doubao-1-5-pro-32k-character-250715": {
                "display_info": {
                    "name": "豆包·1.5·Pro·角色扮演·250715",
                    "description": {
                        "zh_cn": "Doubao-1.5-pro-32k-character-250715，新增故事剧情模式、恋爱拉扯、真人向聊天优化，整体效果提升10~15%",
                        "en_us": "Doubao-1.5-pro-32k-character-250715, with new features such as story plot mode, love pull, and human-like chat optimization, it has a 10~15% improvement in overall performance."
                    },
                    "output_tokens": 4096,
                    "max_tokens": 32768
                },
                "capability": {
                    "function_call": true,
                    "image_understanding": false,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": false
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "生成随机性",
                        "desc": "- **temperature**: 调高温度会使得模型的输出更多样性和创新性，反之，降低温度会使输出内容更加遵循指令要求但减少多样性。建议不要与“Top p”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 1,
                        "default_val": {
                            "default_val": "1",
                            "creative": "1",
                            "balance": "0.8",
                            "precise": "0.3"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "最大回复长度",
                        "desc": "控制模型输出的Tokens 长度上限。通常 100 Tokens 约等于 150 个中文汉字。",
                        "type": 2,
                        "min": "0",
                        "max": "12288",
                        "precision": 0,
                        "default_val": {
                            "default_val": "4096"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "当前时间",
                        "desc": "开启后，会在用户的每次query中拼上当前准确时间。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "SP防泄漏指令",
                        "desc": "开启后将会加固提示词，显著降低提示词泄露情况的出现概率。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    }
                ]
            },
            "doubao-seed-1-6-vision-250815": {
                "display_info": {
                    "name": "豆包·1.6·视觉理解-250815",
                    "description": {
                        "zh_cn": "适用于视频理解、Grounding、GUI Agent等高复杂度的场景，与Doubao-1.5-thinking-vision-pro相比，在教育、图像审核、巡检与安防和AI搜索问答等场景下展现出更强的通用多模态理解和推理能力，支持 256k 上下文窗口，输出长度支持最大 64k tokens。",
                        "en_us": "Doubao-1.6-vision-250815, with higher complexity scenarios such as video understanding, Grounding, GUI Agent, it shows stronger general multimodal understanding and reasoning capabilities compared to Doubao-1.5-thinking-vision-pro. It supports a 256k context window and maximum output length of 64k tokens."
                    },
                    "output_tokens": 4096,
                    "max_tokens": 262144
                },
                "capability": {
                    "cot_display": true,
                    "function_call": true,
                    "image_understanding": true,
                    "video_understanding": true,
                    "audio_understanding": false,
                    "support_multi_modal": true
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "生成随机性",
                        "desc": "- **temperature**: 调高温度会使得模型的输出更多样性和创新性，反之，降低温度会使输出内容更加遵循指令要求但减少多样性。建议不要与“Top p”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "2",
                        "precision": 2,
                        "default_val": {
                            "default_val": "1",
                            "creative": "1",
                            "balance": "0.8",
                            "precise": "0.2"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "top_p",
                        "label": "Top P",
                        "desc": "- **Top p 为累计概率**: 模型在生成输出时会从概率最高的词汇开始选择，直到这些词汇的总概率累积达到Top p 值。这样可以限制模型只选择这些高概率的词汇，从而控制输出内容的多样性。建议不要与“生成随机性”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0.7",
                            "creative": "1",
                            "balance": "1",
                            "precise": "1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "frequency_penalty",
                        "label": "重复语句惩罚",
                        "desc": "- **frequency penalty**: 当该值为正时，会阻止模型频繁使用相同的词汇和短语，从而增加输出内容的多样性。",
                        "type": 1,
                        "min": "-2",
                        "max": "2",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0",
                            "creative": "-2",
                            "balance": "0",
                            "precise": "2"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "最大回复长度",
                        "desc": "控制模型输出的Tokens 长度上限。通常 100 Tokens 约等于 150 个中文汉字。",
                        "type": 2,
                        "min": "0",
                        "max": "32768",
                        "precision": 0,
                        "default_val": {
                            "default_val": "4096"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "当前时间",
                        "desc": "开启后，会在用户的每次query中拼上当前准确时间。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "SP防泄漏指令",
                        "desc": "开启后将会加固提示词，显著降低提示词泄露情况的出现概率。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    },
                    {
                        "name": "thinking_type",
                        "label": "深度思考开关",
                        "desc": "开启深度思考后，在输出最终回答之前，模型会先输出一段思维链内容，以提升最终答案的准确性。",
                        "type": 4,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "enabled"
                        },
                        "options": [
                            {
                                "label": "开启",
                                "value": "enabled"
                            },
                            {
                                "label": "关闭",
                                "value": "disabled"
                            }
                        ],
                        "param_class": {
                            "class_id": 6,
                            "label": "深度思考"
                        }
                    }
                ]
            },
            "seed_strong_character": {
                "display_info": {
                    "name": "豆包·角色扮演",
                    "description": {
                        "zh_cn": "Seed-strong-character，通过深入分析用户的输入和行为，制定个性化的响应策略，能够灵活地适应不同角色和情境。目前该模型不支持付费扩充额度。",
                        "en_us": "Seed-strong-character, through deep analysis of user input and behavior, it can develop personalized response strategies that can flexibly adapt to different roles and scenarios. Currently, this model does not support paid extended quota."
                    },
                    "output_tokens": 4096,
                    "max_tokens": 32768
                },
                "capability": {
                    "function_call": false,
                    "image_understanding": false,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": false
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "生成随机性",
                        "desc": "- **temperature**: 调高温度会使得模型的输出更多样性和创新性，反之，降低温度会使输出内容更加遵循指令要求但减少多样性。建议不要与“Top p”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 1,
                        "default_val": {
                            "default_val": "1",
                            "creative": "1",
                            "balance": "1",
                            "precise": "1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "top_p",
                        "label": "Top P",
                        "desc": "- **Top p 为累计概率**: 模型在生成输出时会从概率最高的词汇开始选择，直到这些词汇的总概率累积达到Top p 值。这样可以限制模型只选择这些高概率的词汇，从而控制输出内容的多样性。建议不要与“生成随机性”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0.7",
                            "creative": "0.7",
                            "balance": "0.7",
                            "precise": "0.7"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "最大回复长度",
                        "desc": "控制模型输出的Tokens 长度上限。通常 100 Tokens 约等于 150 个中文汉字。",
                        "type": 2,
                        "min": "1",
                        "max": "32768",
                        "precision": 0,
                        "default_val": {
                            "default_val": "2000"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "当前时间",
                        "desc": "开启后，会在用户的每次query中拼上当前准确时间。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "SP防泄漏指令",
                        "desc": "开启后将会加固提示词，显著降低提示词泄露情况的出现概率。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    }
                ]
            }
        },
        "Llama": {
            "default": {
                "display_info": {
                    "output_tokens": 4096,
                    "max_tokens": 8192
                },
                "capability": {
                    "function_call": true,
                    "image_understanding": false,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": false
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "生成随机性",
                        "desc": "- **temperature**: 调高温度会使得模型的输出更多样性和创新性，反之，降低温度会使输出内容更加遵循指令要求但减少多样性。建议不要与“Top p”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0.85",
                            "creative": "0.95",
                            "balance": "0.85",
                            "precise": "0.1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "最大回复长度",
                        "desc": "控制模型输出的Tokens 长度上限。通常 100 Tokens 约等于 150 个中文汉字。",
                        "type": 2,
                        "min": "5",
                        "max": "2000",
                        "precision": 0,
                        "default_val": {
                            "default_val": "2000"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    }
                ]
            }
        },
        "DeekSeek": {
            "default": {
                "display_info": {
                    "output_tokens": 4096,
                    "max_tokens": 8192
                },
                "capability": {
                    "function_call": true,
                    "image_understanding": false,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": false
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "生成随机性",
                        "desc": "- **temperature**: 调高温度会使得模型的输出更多样性和创新性，反之，降低温度会使输出内容更加遵循指令要求但减少多样性。建议不要与“Top p”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0.85",
                            "creative": "0.95",
                            "balance": "0.85",
                            "precise": "0.1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "最大回复长度",
                        "desc": "控制模型输出的Tokens 长度上限。通常 100 Tokens 约等于 150 个中文汉字。",
                        "type": 2,
                        "min": "5",
                        "max": "2000",
                        "precision": 0,
                        "default_val": {
                            "default_val": "2000"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    }
                ]
            },
            "deepseek-reasoner": {
                "display_info": {
                    "name": "DeepSeek-R1·工具调用",
                    "description": {
                        "zh_cn": "R1 functionCall 版本，支持在Single-Agent模式下调用各类扣子工具（插件、工作流、知识库等）。",
                        "en_us": "DeepSeek-R1 functionCall version, which supports calling various Coze tools (plugins, workflows, knowledge bases, etc.) in Single-Agent mode."
                    },
                    "output_tokens": 4096,
                    "max_tokens": 8192
                },
                "capability": {
                    "function_call": true,
                    "image_understanding": false,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": false
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "生成随机性",
                        "desc": "- **temperature**: 调高温度会使得模型的输出更多样性和创新性，反之，降低温度会使输出内容更加遵循指令要求但减少多样性。建议不要与“Top p”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 1,
                        "default_val": {
                            "default_val": "1",
                            "creative": "1",
                            "balance": "0.8",
                            "precise": "0.3"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "最大回复长度",
                        "desc": "控制模型输出的Tokens 长度上限。通常 100 Tokens 约等于 150 个中文汉字。",
                        "type": 2,
                        "min": "1",
                        "max": "8192",
                        "precision": 0,
                        "default_val": {
                            "default_val": "2200"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "当前时间",
                        "desc": "开启后，会在用户的每次query中拼上当前准确时间。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "SP防泄漏指令",
                        "desc": "开启后将会加固提示词，显著降低提示词泄露情况的出现概率。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    }
                ]
            },
            "deepseek-chat": {
                "display_info": {
                    "name": "DeepSeek-V3·工具调用",
                    "description": {
                        "zh_cn": "V3 functionCall 版本，支持在Single-Agent模式下调用各类扣子工具（插件、工作流、知识库等）。",
                        "en_us": "DeepSeek-V3 functionCall version, which supports calling various Coze tools (plugins, workflows, knowledge bases, etc.) in Single-Agent mode."
                    },
                    "output_tokens": 4096,
                    "max_tokens": 65536
                },
                "capability": {
                    "function_call": true,
                    "image_understanding": false,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": false
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "生成随机性",
                        "desc": "- **temperature**: 调高温度会使得模型的输出更多样性和创新性，反之，降低温度会使输出内容更加遵循指令要求但减少多样性。建议不要与“Top p”同时调整。",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 1,
                        "default_val": {
                            "default_val": "1",
                            "creative": "1",
                            "balance": "0.8",
                            "precise": "0.3"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多样性"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "最大回复长度",
                        "desc": "控制模型输出的Tokens 长度上限。通常 100 Tokens 约等于 150 个中文汉字。",
                        "type": 2,
                        "min": "1",
                        "max": "8192",
                        "precision": 0,
                        "default_val": {
                            "default_val": "1024"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "输入及输出设置"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "当前时间",
                        "desc": "开启后，会在用户的每次query中拼上当前准确时间。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "SP防泄漏指令",
                        "desc": "开启后将会加固提示词，显著降低提示词泄露情况的出现概率。[指引文档](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默认指令"
                        }
                    }
                ]
            }
        }
    }
}